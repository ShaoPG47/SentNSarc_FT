{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376c8d2a",
   "metadata": {},
   "source": [
    "## Fine-Tune Stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c07eb9",
   "metadata": {},
   "source": [
    "### Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb803089",
   "metadata": {},
   "source": [
    "### Debug Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797a51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | GPU: NVIDIA GeForce RTX 4090 | VRAM (GB): 23.64\n",
      "[REPLAY] sentiment sampled = 1000\n",
      "[MT-TRAIN] total=2000 | frac sarcasm=0.500\n",
      "[SENT] train_replay=1000, val=300\n",
      "[SARC] train=1000, val=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/venvs/llamaFT_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 15045.27 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 13697.62 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 16955.13 examples/s]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 12.40it/s]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/exx/venvs/llamaFT_env/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 20/62 [00:11<00:23,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7835, 'grad_norm': 71.5, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 40/62 [00:22<00:12,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7836, 'grad_norm': 67.5, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 60/62 [00:34<00:01,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7531, 'grad_norm': 24.875, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:35<00:00,  1.75it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:40<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0, 'eval_accuracy': 0.81, 'eval_macro_f1': 0.7866685629437175, 'eval_runtime': 5.04, 'eval_samples_per_second': 59.523, 'eval_steps_per_second': 7.54, 'epoch': 0.99}\n",
      "{'train_runtime': 40.3986, 'train_samples_per_second': 49.507, 'train_steps_per_second': 1.535, 'train_loss': 0.7704854165354083, 'epoch': 0.99}\n",
      "\n",
      "[STAGE-2] train done (debug mode = True ).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:04<00:00,  8.05it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:04<00:00,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SENTIMENT | validation | N=300 ===\n",
      "Accuracy: 0.8100 | Macro-F1: 0.7867\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8586    0.8173    0.8374       104\n",
      "     neutral     0.5897    0.6765    0.6301        68\n",
      "    positive     0.9106    0.8750    0.8924       128\n",
      "\n",
      "    accuracy                         0.8100       300\n",
      "   macro avg     0.7863    0.7896    0.7867       300\n",
      "weighted avg     0.8198    0.8100    0.8139       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:04<00:00,  8.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:04<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SARCASM | validation | N=300 ===\n",
      "Accuracy: 0.3833 | Macro-F1: 0.3650\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "non_sarcastic     0.3902    0.1916    0.2570       167\n",
      "    sarcastic     0.3807    0.6241    0.4729       133\n",
      "\n",
      "     accuracy                         0.3833       300\n",
      "    macro avg     0.3855    0.4078    0.3650       300\n",
      " weighted avg     0.3860    0.3833    0.3527       300\n",
      "\n",
      "\n",
      "== Saved Stage-2 artifacts ==\n",
      "Adapter dir: /media/veracrypt1/DS5500/project/stage2/stage2Result/output/adapter\n",
      "Sentiment head: /media/veracrypt1/DS5500/project/stage2/stage2Result/output/sent_head.pt\n",
      "Sarcasm head: /media/veracrypt1/DS5500/project/stage2/stage2Result/output/sarc_head.pt\n",
      "Sentiment results @ /media/veracrypt1/DS5500/project/stage2/stage2Result/sent_result\n",
      "Sarcasm   results @ /media/veracrypt1/DS5500/project/stage2/stage2Result/sarc_result\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Stage 2: Multi-task Fine-tuning (Sentiment + Sarcasm) [DEBUG MODE READY]\n",
    "#   - Reuse Stage-1 LoRA + trained 3-way sentiment head\n",
    "#   - Add a new 2-way sarcasm head\n",
    "#   - Loss = CE_sarc + Î» * CE_sent (masked by task)\n",
    "#   - Save artifacts to stage2/stage2Result/{sent_result,sarc_result,output}\n",
    "# =======================\n",
    "\n",
    "import os, json, random, numpy as np, pandas as pd, torch\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Basic env ----------\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"0\")\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "assert torch.cuda.is_available(), \"CUDA not found.\"\n",
    "\n",
    "SEED = 47\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ---------- Paths ----------\n",
    "PROJECT_ROOT   = Path(\"/media/veracrypt1/DS5500/project\")\n",
    "SENT_CSV       = PROJECT_ROOT / \"data\" / \"processed\" / \"sentiment_df.csv\"\n",
    "SARC_CSV       = PROJECT_ROOT / \"data\" / \"processed\" / \"sarcasm_df.csv\"\n",
    "# âœ… Reuse Stage-1 artifacts saved under stage1Result (as you required)\n",
    "STAGE1_DIR     = PROJECT_ROOT / \"stage1\" / \"stage1Result\" / \"output\"\n",
    "STAGE1_ADAPTER = STAGE1_DIR / \"adapter\"\n",
    "STAGE1_HEAD_PT = STAGE1_DIR / \"sent_head.pt\"\n",
    "\n",
    "STAGE2_DIR     = PROJECT_ROOT / \"stage2\"\n",
    "STAGE2_WORK    = STAGE2_DIR / \"stage2_multitask\"     # working dir for Trainer outputs\n",
    "STAGE2_RESULT  = STAGE2_DIR / \"stage2Result\"         # result tree required by you\n",
    "SENT_RESULT_DIR = STAGE2_RESULT / \"sent_result\"\n",
    "SARC_RESULT_DIR = STAGE2_RESULT / \"sarc_result\"\n",
    "OUTPUT_DIR      = STAGE2_RESULT / \"output\"\n",
    "for d in [STAGE2_WORK, SENT_RESULT_DIR, SARC_RESULT_DIR, OUTPUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "print(\"Device: cuda | GPU:\", torch.cuda.get_device_name(0),\n",
    "      \"| VRAM (GB):\", round(torch.cuda.get_device_properties(0).total_memory/1024**3,2))\n",
    "\n",
    "# ---------- Debug switch (sample tiny subsets to fix pipeline first) ----------\n",
    "DEBUG_SMALL = True\n",
    "SARC_DEBUG_N = 1000\n",
    "SENT_DEBUG_N = 1000\n",
    "VAL_DEBUG_N  = 300   # per task for faster debug\n",
    "\n",
    "# ---------- Read data ----------\n",
    "def norm_split(s):\n",
    "    s = str(s).strip().lower()\n",
    "    if s in {\"val\",\"valid\",\"validation\",\"dev\"}: return \"validation\"\n",
    "    if s in {\"train\",\"training\"}: return \"train\"\n",
    "    if s in {\"test\",\"testing\"}:  return \"test\"\n",
    "    return s\n",
    "\n",
    "# Sentiment\n",
    "sent_df_all = pd.read_csv(SENT_CSV)\n",
    "assert {\"text\",\"sentiment\",\"split\"}.issubset(sent_df_all.columns)\n",
    "sent_df_all[\"split_norm\"] = sent_df_all[\"split\"].map(norm_split)\n",
    "sent_train_df = sent_df_all[sent_df_all[\"split_norm\"]==\"train\"][[\"text\",\"sentiment\"]].rename(columns={\"sentiment\":\"label_sent\"}).reset_index(drop=True)\n",
    "sent_val_df   = sent_df_all[sent_df_all[\"split_norm\"]==\"validation\"][[\"text\",\"sentiment\"]].rename(columns={\"sentiment\":\"label_sent\"}).reset_index(drop=True)\n",
    "sent_train_df[\"label_sent\"] = sent_train_df[\"label_sent\"].astype(int)\n",
    "sent_val_df[\"label_sent\"]   = sent_val_df[\"label_sent\"].astype(int)\n",
    "\n",
    "# Sarcasm\n",
    "sarc_df_all = pd.read_csv(SARC_CSV)\n",
    "assert \"text\" in sarc_df_all.columns and \"split\" in sarc_df_all.columns\n",
    "if \"sarcasm\" in sarc_df_all.columns:\n",
    "    sarc_df_all = sarc_df_all.rename(columns={\"sarcasm\":\"label_sarc\"})\n",
    "elif \"label\" in sarc_df_all.columns:\n",
    "    sarc_df_all = sarc_df_all.rename(columns={\"label\":\"label_sarc\"})\n",
    "else:\n",
    "    raise ValueError(\"Sarcasm csv must have 'sarcasm' or 'label' column.\")\n",
    "sarc_df_all[\"split_norm\"] = sarc_df_all[\"split\"].map(norm_split)\n",
    "sarc_train_df = sarc_df_all[sarc_df_all[\"split_norm\"]==\"train\"][[\"text\",\"label_sarc\"]].reset_index(drop=True)\n",
    "sarc_val_df   = sarc_df_all[sarc_df_all[\"split_norm\"]==\"validation\"][[\"text\",\"label_sarc\"]].reset_index(drop=True)\n",
    "sarc_train_df[\"label_sarc\"] = sarc_train_df[\"label_sarc\"].astype(int)\n",
    "sarc_val_df[\"label_sarc\"]   = sarc_val_df[\"label_sarc\"].astype(int)\n",
    "\n",
    "# ---------- Build replay (sentiment) + sarcasm multi-task train ----------\n",
    "LAMBDA_SENT  = 0.5\n",
    "REPLAY_RATIO = 1.0\n",
    "\n",
    "replay_target = int(REPLAY_RATIO * len(sarc_train_df))\n",
    "g = sent_train_df.groupby(\"label_sent\", group_keys=False)\n",
    "per_cls = max(1, replay_target // max(1, g.size().shape[0]))\n",
    "replay_sent_df = g.apply(lambda x: x.sample(n=min(per_cls, len(x)), random_state=SEED),\n",
    "                         include_groups=False).reset_index(drop=True)\n",
    "if len(replay_sent_df) < replay_target and len(sent_train_df) > len(replay_sent_df):\n",
    "    need = replay_target - len(replay_sent_df)\n",
    "    replay_sent_df = pd.concat([replay_sent_df, sent_train_df.sample(n=need, random_state=SEED)], ignore_index=True)\n",
    "replay_sent_df = replay_sent_df.sample(frac=1.0, random_state=SEED).reset_index(drop=Trudddddde)\n",
    "\n",
    "# ---------- DEBUG subset sampling ----------\n",
    "if DEBUG_SMALL:\n",
    "    sarc_train_df = sarc_train_df.sample(n=min(SARC_DEBUG_N, len(sarc_train_df)), random_state=SEED).reset_index(drop=True)\n",
    "    replay_sent_df = replay_sent_df.sample(n=min(SENT_DEBUG_N, len(replay_sent_df)), random_state=SEED).reset_index(drop=True)\n",
    "    sent_val_df = sent_val_df.sample(n=min(VAL_DEBUG_N, len(sent_val_df)), random_state=SEED).reset_index(drop=True)\n",
    "    sarc_val_df = sarc_val_df.sample(n=min(VAL_DEBUG_N, len(sarc_val_df)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "mt_train_df = pd.DataFrame({\n",
    "    \"text\": pd.concat([sarc_train_df[\"text\"], replay_sent_df[\"text\"]], ignore_index=True),\n",
    "})\n",
    "mt_train_df[\"label_sarc\"] = pd.concat([sarc_train_df[\"label_sarc\"], pd.Series([-100]*len(replay_sent_df))], ignore_index=True)\n",
    "mt_train_df[\"label_sent\"] = pd.concat([pd.Series([-100]*len(sarc_train_df)), replay_sent_df[\"label_sent\"]], ignore_index=True)\n",
    "mt_train_df[\"task_id\"]    = (mt_train_df[\"label_sarc\"] != -100).astype(int)  # 1=sarc, 0=sent\n",
    "\n",
    "both_mask = (mt_train_df[\"label_sent\"]==-100) & (mt_train_df[\"label_sarc\"]==-100)\n",
    "assert not both_mask.any(), f\"Bad rows with both labels -100: {both_mask.sum()}\"\n",
    "\n",
    "# Eval sets: each single-task with 'labels' column for HF\n",
    "sent_val_df = sent_val_df[[\"text\",\"label_sent\"]].reset_index(drop=True)\n",
    "sarc_val_df = sarc_val_df[[\"text\",\"label_sarc\"]].reset_index(drop=True)\n",
    "\n",
    "print(f\"[REPLAY] sentiment sampled = {len(replay_sent_df)}\")\n",
    "print(f\"[MT-TRAIN] total={len(mt_train_df)} | frac sarcasm={mt_train_df['task_id'].mean():.3f}\")\n",
    "print(f\"[SENT] train_replay={len(replay_sent_df)}, val={len(sent_val_df)}\")\n",
    "print(f\"[SARC] train={len(sarc_train_df)}, val={len(sarc_val_df)}\")\n",
    "\n",
    "# ---------- Tokenization ----------\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tok.padding_side = \"right\"; tok.truncation_side = \"right\"\n",
    "if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "def tok_mt_train(batch):\n",
    "    enc = tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "    def nz(x):  # NaN -> -100\n",
    "        return -100 if (x is None or (isinstance(x, float) and np.isnan(x))) else int(x)\n",
    "    enc[\"labels_sent\"] = [nz(x) for x in batch[\"label_sent\"]]\n",
    "    enc[\"labels_sarc\"] = [nz(x) for x in batch[\"label_sarc\"]]\n",
    "    enc[\"task_id\"]     = [int(x) for x in batch[\"task_id\"]]\n",
    "    return enc\n",
    "\n",
    "def tok_eval_sent(batch):\n",
    "    enc = tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "    enc[\"labels\"]  = [int(x) for x in batch[\"label_sent\"]]\n",
    "    enc[\"task_id\"] = [0]*len(batch[\"label_sent\"])\n",
    "    return enc\n",
    "\n",
    "def tok_eval_sarc(batch):\n",
    "    enc = tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "    enc[\"labels\"]  = [int(x) for x in batch[\"label_sarc\"]]\n",
    "    enc[\"task_id\"] = [1]*len(batch[\"label_sarc\"])\n",
    "    return enc\n",
    "\n",
    "mt_train_tok = Dataset.from_pandas(mt_train_df).map(\n",
    "    tok_mt_train, batched=True, remove_columns=list(mt_train_df.columns)\n",
    ").with_format(\"torch\")\n",
    "sent_val_tok  = Dataset.from_pandas(sent_val_df).map(\n",
    "    tok_eval_sent, batched=True, remove_columns=list(sent_val_df.columns)\n",
    ").with_format(\"torch\")\n",
    "sarc_val_tok  = Dataset.from_pandas(sarc_val_df).map(\n",
    "    tok_eval_sarc, batched=True, remove_columns=list(sarc_val_df.columns)\n",
    ").with_format(\"torch\")\n",
    "\n",
    "# ---------- Load Stage-1 base + LoRA + trained 3-way head ----------\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "from torch import nn\n",
    "\n",
    "use_bf16    = torch.cuda.is_bf16_supported()\n",
    "torch_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "# base CLS model (3-way) + LoRA adapter\n",
    "base_seqcls = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# attach Stage-1 LoRA adapter\n",
    "base_seqcls = PeftModel.from_pretrained(base_seqcls, STAGE1_ADAPTER)\n",
    "\n",
    "# âœ… load the trained Stage-1 3-way head\n",
    "assert STAGE1_HEAD_PT.exists(), f\"Missing Stage-1 head: {STAGE1_HEAD_PT}\"\n",
    "head_state = torch.load(STAGE1_HEAD_PT, map_location=\"cpu\")\n",
    "base_seqcls.score.load_state_dict(head_state)\n",
    "\n",
    "# ---------- Multi-task wrapper ----------\n",
    "class MultiTaskSentSarc(nn.Module):\n",
    "    \"\"\"\n",
    "    Reuse Stage-1 peft_seqcls (with a trained 3-way head) + add a new 2-way sarcasm head.\n",
    "    Masked loss: CE over available labels, loss = CE_sarc + lambda * CE_sent.\n",
    "    \"\"\"\n",
    "    def __init__(self, peft_seqcls_model: nn.Module, pad_token_id: int, lambda_sent: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.peft_seqcls = peft_seqcls_model\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.lambda_sent = lambda_sent\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        # dtype/device alignment for the new head\n",
    "        sample_param = next(self.peft_seqcls.parameters())\n",
    "        hidden = self.peft_seqcls.config.hidden_size\n",
    "        self.sarc_head = nn.Linear(hidden, 2, device=sample_param.device, dtype=sample_param.dtype)\n",
    "        # ensure hidden states are returned\n",
    "        self.peft_seqcls.config.output_hidden_states = True\n",
    "\n",
    "    def _pool_last_nonpad(self, hidden_states, attention_mask):\n",
    "        if attention_mask is None:\n",
    "            return hidden_states[:, -1, :]\n",
    "        idx = attention_mask.long().sum(dim=1) - 1\n",
    "        idx = torch.clamp(idx, min=0)\n",
    "        return hidden_states[torch.arange(hidden_states.size(0), device=hidden_states.device), idx]\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,\n",
    "                labels_sent=None, labels_sarc=None, task_id=None, **kwargs):\n",
    "        # unwrap backbone\n",
    "        if hasattr(self.peft_seqcls, \"get_base_model\"):\n",
    "            backbone = self.peft_seqcls.get_base_model().model\n",
    "        else:\n",
    "            backbone = getattr(getattr(self.peft_seqcls, \"base_model\", self.peft_seqcls), \"model\", self.peft_seqcls)\n",
    "\n",
    "        out = backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        hidden = out.last_hidden_state if hasattr(out, \"last_hidden_state\") else out.hidden_states[-1]\n",
    "        pooled = self._pool_last_nonpad(hidden, attention_mask)\n",
    "\n",
    "        logits_sent = self.peft_seqcls.score(pooled)             # [B,3]\n",
    "        logits_sarc = self.sarc_head(pooled)                     # [B,2]\n",
    "\n",
    "        # masked multi-task loss\n",
    "        loss = None\n",
    "        if labels_sent is not None and (labels_sent != -100).any():\n",
    "            m = (labels_sent != -100)\n",
    "            loss_sent = self.ce(logits_sent[m], labels_sent[m]).mean()\n",
    "            loss = (self.lambda_sent * loss_sent) if loss is None else (loss + self.lambda_sent * loss_sent)\n",
    "        if labels_sarc is not None and (labels_sarc != -100).any():\n",
    "            m = (labels_sarc != -100)\n",
    "            loss_sarc = self.ce(logits_sarc[m], labels_sarc[m]).mean()\n",
    "            loss = loss_sarc if loss is None else (loss + loss_sarc)\n",
    "        if loss is None:\n",
    "            loss = (logits_sent.mean()*0) + (logits_sarc.mean()*0)\n",
    "\n",
    "        # for eval convenience: if the whole batch is sarcasm, switch logits to 2-way\n",
    "        logits = logits_sent\n",
    "        if task_id is not None and torch.all(task_id == 1):\n",
    "            logits = logits_sarc\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits,\n",
    "                \"logits_sent\": logits_sent, \"logits_sarc\": logits_sarc}\n",
    "\n",
    "mt_model = MultiTaskSentSarc(base_seqcls, pad_token_id=tok.pad_token_id, lambda_sent=LAMBDA_SENT).to(\"cuda\")\n",
    "\n",
    "# ---------- Custom Trainer ----------\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "class MTTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # pop our custom labels; keep 'labels' for eval only\n",
    "        labels_sent = inputs.pop(\"labels_sent\", None)\n",
    "        labels_sarc = inputs.pop(\"labels_sarc\", None)\n",
    "        outputs = model(**inputs, labels_sent=labels_sent, labels_sarc=labels_sarc)\n",
    "        loss = outputs.get(\"loss\", None)\n",
    "        if loss is None:\n",
    "            ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "            ls = lr = 0.0; count = 0\n",
    "            if labels_sarc is not None and (labels_sarc != -100).any():\n",
    "                m = (labels_sarc != -100)\n",
    "                lr = ce(outputs[\"logits_sarc\"][m], labels_sarc[m]).mean()\n",
    "                loss = lr; count += 1\n",
    "            if labels_sent is not None and (labels_sent != -100).any():\n",
    "                m = (labels_sent != -100)\n",
    "                ls = ce(outputs[\"logits_sent\"][m], labels_sent[m]).mean()\n",
    "                loss = (loss if count else 0) + getattr(model, \"lambda_sent\", 0.5) * ls\n",
    "            if loss is None:\n",
    "                loss = outputs[\"logits\"].mean()*0\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    # Ensure metrics see a single (N, C) logits tensor\n",
    "    def preprocess_logits_for_metrics(self, logits, labels):\n",
    "        if isinstance(logits, (tuple, list)):\n",
    "            return logits[0]\n",
    "        return logits\n",
    "\n",
    "# Metrics for single-task eval sets (labels = 0/1/2 or 0/1)\n",
    "def basic_metrics(p):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, (tuple,list)) else p.predictions\n",
    "    pred = np.argmax(preds, axis=-1)\n",
    "    return {\"accuracy\": float(accuracy_score(p.label_ids, pred)),\n",
    "            \"macro_f1\": float(f1_score(p.label_ids, pred, average=\"macro\"))}\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM = 4\n",
    "\n",
    "mt_args = TrainingArguments(\n",
    "    output_dir=str(STAGE2_WORK),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50 if DEBUG_SMALL else 100,\n",
    "    num_train_epochs=1 if DEBUG_SMALL else 4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    logging_steps=20 if DEBUG_SMALL else 50,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=8,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    # âœ… CRITICAL: keep ONLY \"labels\" here; do not include labels_sent/labels_sarc\n",
    "    label_names=[\"labels\"],\n",
    "    save_safetensors=False,\n",
    ")\n",
    "\n",
    "mt_trainer = MTTrainer(\n",
    "    model=mt_model,\n",
    "    args=mt_args,\n",
    "    train_dataset=mt_train_tok,\n",
    "    eval_dataset=sent_val_tok,     # default eval = sentiment; we will evaluate both tasks below\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=basic_metrics,\n",
    ")\n",
    "\n",
    "# ---------- Train ----------\n",
    "train_result = mt_trainer.train()\n",
    "print(\"\\n[STAGE-2] train done (debug mode =\", DEBUG_SMALL, \").\")\n",
    "\n",
    "# ---------- Eval helper (safe against pred.label_ids=None) ----------\n",
    "def eval_and_save(name, eval_ds, out_dir, n_classes, id2label):\n",
    "    \"\"\"\n",
    "    - Uses trainer.evaluate for loss + metrics (compute_metrics uses label_ids provided internally).\n",
    "    - Uses trainer.predict for per-sample outputs, but y_true is read directly from eval_ds[\"labels\"] to be robust.\n",
    "    - Saves confusion matrix, classification report, per-sample CSV, raw logits.npy.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) evaluate() â€“ gives loss, and calls compute_metrics (which uses label_ids extracted by Trainer)\n",
    "    r = mt_trainer.evaluate(eval_dataset=eval_ds, metric_key_prefix=name)\n",
    "    with open(out_dir / \"metrics_eval.json\", \"w\") as f:\n",
    "        json.dump({k: float(v) if isinstance(v, (int,float,np.floating)) else v for k,v in r.items()}, f, indent=2)\n",
    "\n",
    "    # 2) predict() â€“ robustly get logits and y_true\n",
    "    pred = mt_trainer.predict(eval_ds)\n",
    "    logits = pred.predictions[0] if isinstance(pred.predictions, (tuple, list)) else pred.predictions\n",
    "    y_pred = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # read gold labels directly from dataset (robust to HF quirks)\n",
    "    y_true = np.array(eval_ds[\"labels\"], dtype=int)\n",
    "\n",
    "    # metrics again (manual)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    cm  = confusion_matrix(y_true, y_pred, labels=list(range(n_classes)))\n",
    "\n",
    "    # save confusion matrix\n",
    "    cm_df = pd.DataFrame(cm, index=[id2label[i] for i in range(n_classes)],\n",
    "                            columns=[id2label[i] for i in range(n_classes)])\n",
    "    cm_df.to_csv(out_dir / \"confusion_matrix.csv\", index=True)\n",
    "\n",
    "    # save classification report\n",
    "    report_str = classification_report(y_true, y_pred,\n",
    "                                       target_names=[id2label[i] for i in range(n_classes)],\n",
    "                                       digits=4)\n",
    "    with open(out_dir / \"classification_report.txt\", \"w\") as f:\n",
    "        f.write(report_str)\n",
    "\n",
    "    # save per-sample with text\n",
    "    # need original text side-by-side:\n",
    "    if name == \"sentiment\":\n",
    "        src_df = sent_val_df.copy()\n",
    "        true_map_col = \"label_sent\"\n",
    "    else:\n",
    "        src_df = sarc_val_df.copy()\n",
    "        true_map_col = \"label_sarc\"\n",
    "\n",
    "    per_sample = pd.DataFrame({\n",
    "        \"pred_label\": y_pred,\n",
    "        \"pred_name\":  [id2label[int(i)] for i in y_pred],\n",
    "        \"true_label\": y_true,\n",
    "        \"true_name\":  [id2label[int(i)] for i in y_true],\n",
    "    })\n",
    "    # align lengths if sampling caused minor length drift\n",
    "    L = min(len(src_df), len(per_sample))\n",
    "    src_df = src_df.iloc[:L].reset_index(drop=True)\n",
    "    per_sample = per_sample.iloc[:L].reset_index(drop=True)\n",
    "    merged = pd.concat([src_df, per_sample], axis=1)\n",
    "    merged.to_csv(out_dir / \"val_predictions_with_text.csv\", index=False)\n",
    "\n",
    "    # save raw logits\n",
    "    np.save(out_dir / \"val_logits.npy\", logits)\n",
    "\n",
    "    # console print\n",
    "    print(f\"\\n=== {name.upper()} | validation | N={len(y_true)} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Macro-F1: {mf1:.4f}\")\n",
    "    print(report_str)\n",
    "\n",
    "# ---------- Evaluate (two tasks) ----------\n",
    "id2label_sent = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\n",
    "id2label_sarc = {0:\"non_sarcastic\", 1:\"sarcastic\"}\n",
    "\n",
    "eval_and_save(\"sentiment\", sent_val_tok, SENT_RESULT_DIR, n_classes=3, id2label=id2label_sent)\n",
    "eval_and_save(\"sarcasm\",   sarc_val_tok, SARC_RESULT_DIR, n_classes=2, id2label=id2label_sarc)\n",
    "\n",
    "# ---------- Save adapters + heads + configs ----------\n",
    "# Save peft adapter (LoRA) as used in Stage-2 (same adapter, but we snapshot to Stage-2 result tree)\n",
    "ADAPTER_DIR = OUTPUT_DIR / \"adapter\"\n",
    "ADAPTER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# base_seqcls is inside mt_model.peft_seqcls; it is a PeftModel-wrapped SeqCls\n",
    "mt_model.peft_seqcls.save_pretrained(ADAPTER_DIR)\n",
    "tok.save_pretrained(ADAPTER_DIR)\n",
    "\n",
    "# Save both heads (.pt)\n",
    "torch.save(mt_model.peft_seqcls.score.state_dict(), OUTPUT_DIR / \"sent_head.pt\")\n",
    "torch.save(mt_model.sarc_head.state_dict(),        OUTPUT_DIR / \"sarc_head.pt\")\n",
    "\n",
    "# Save run meta\n",
    "with open(OUTPUT_DIR / \"run_meta_stage2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"seed\": SEED,\n",
    "        \"debug_mode\": DEBUG_SMALL,\n",
    "        \"lambda_sent\": LAMBDA_SENT,\n",
    "        \"replay_ratio\": REPLAY_RATIO,\n",
    "        \"train_sizes\": {\n",
    "            \"sarc_train\": int(len(sarc_train_df)),\n",
    "            \"sent_replay_train\": int(len(replay_sent_df)),\n",
    "            \"mt_train_total\": int(len(mt_train_df)),\n",
    "        },\n",
    "        \"val_sizes\": {\n",
    "            \"sent_val\": int(len(sent_val_df)),\n",
    "            \"sarc_val\": int(len(sarc_val_df)),\n",
    "        },\n",
    "        \"id2label_sent\": id2label_sent,\n",
    "        \"id2label_sarc\": id2label_sarc,\n",
    "        \"stage1_adapter_dir\": str(STAGE1_ADAPTER),\n",
    "        \"stage1_head_pt\": str(STAGE1_HEAD_PT)\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n== Saved Stage-2 artifacts ==\")\n",
    "print(\"Adapter dir:\", ADAPTER_DIR)\n",
    "print(\"Sentiment head:\", OUTPUT_DIR / \"sent_head.pt\")\n",
    "print(\"Sarcasm head:\",   OUTPUT_DIR / \"sarc_head.pt\")\n",
    "print(\"Sentiment results @\", SENT_RESULT_DIR)\n",
    "print(\"Sarcasm   results @\", SARC_RESULT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa32659",
   "metadata": {},
   "source": [
    "### Formal Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f277a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | GPU: NVIDIA GeForce RTX 4090 | VRAM (GB): 23.64\n",
      "[REPLAY] sentiment sampled = 11009\n",
      "[MT-TRAIN] total=22018 | frac sarcasm=0.500\n",
      "[SENT] train_replay=11009, val=8101\n",
      "[SARC] train=11009, val=955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/venvs/llamaFT_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22018/22018 [00:01<00:00, 15393.25 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8101/8101 [00:00<00:00, 13087.50 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 955/955 [00:00<00:00, 19739.70 examples/s]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 15.96it/s]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/exx/venvs/llamaFT_env/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  2%|â–         | 50/2752 [00:28<25:29,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8179, 'grad_norm': 27.5, 'learning_rate': 1e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–Ž         | 100/2752 [00:56<24:48,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7319, 'grad_norm': 30.125, 'learning_rate': 2e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 150/2752 [01:24<24:35,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6329, 'grad_norm': 31.625, 'learning_rate': 1.998246378962966e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 200/2752 [01:53<23:58,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6551, 'grad_norm': 15.3125, 'learning_rate': 1.992991666225347e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 250/2752 [02:21<23:34,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6475, 'grad_norm': 28.25, 'learning_rate': 1.984254291336743e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 300/2752 [02:49<23:02,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6505, 'grad_norm': 43.25, 'learning_rate': 1.972064898385981e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 350/2752 [03:18<22:31,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6196, 'grad_norm': 6.53125, 'learning_rate': 1.9564662385248743e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–        | 400/2752 [03:46<22:16,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6428, 'grad_norm': 7.09375, 'learning_rate': 1.937513020029588e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–‹        | 450/2752 [04:14<21:28,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6191, 'grad_norm': 31.875, 'learning_rate': 1.9152717164254668e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 500/2752 [04:42<21:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6365, 'grad_norm': 13.5, 'learning_rate': 1.889820333348294e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 550/2752 [05:11<20:40,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6387, 'grad_norm': 10.0, 'learning_rate': 1.8612481349596406e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 600/2752 [05:39<20:34,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6232, 'grad_norm': 17.625, 'learning_rate': 1.829655330875844e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–Ž       | 650/2752 [06:08<19:51,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6403, 'grad_norm': 7.03125, 'learning_rate': 1.7951527247086243e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 688/2752 [06:29<19:39,  1.75it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "                                                  \n",
      " 25%|â–ˆâ–ˆâ–Œ       | 688/2752 [08:40<19:39,  1.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0, 'eval_accuracy': 0.7970620910998643, 'eval_macro_f1': 0.7857010300256467, 'eval_runtime': 130.6845, 'eval_samples_per_second': 61.989, 'eval_steps_per_second': 7.751, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 700/2752 [08:47<46:03,  1.35s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6128, 'grad_norm': 14.75, 'learning_rate': 1.757861325449997e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 750/2752 [09:15<18:44,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.628, 'grad_norm': 6.03125, 'learning_rate': 1.717911923064442e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 800/2752 [09:43<18:14,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6186, 'grad_norm': 23.625, 'learning_rate': 1.6754446297768404e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆ       | 850/2752 [10:11<17:42,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6318, 'grad_norm': 7.84375, 'learning_rate': 1.6306083886649823e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 900/2752 [10:40<17:27,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6126, 'grad_norm': 22.125, 'learning_rate': 1.5835604512801375e-05, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 950/2752 [11:08<17:09,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6015, 'grad_norm': 38.0, 'learning_rate': 1.5344658261278013e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1000/2752 [11:36<16:29,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5957, 'grad_norm': 12.375, 'learning_rate': 1.4834966999429179e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1050/2752 [12:04<15:54,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6055, 'grad_norm': 6.625, 'learning_rate': 1.4308318337893214e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1100/2752 [12:33<15:22,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.608, 'grad_norm': 13.25, 'learning_rate': 1.3766559361014113e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1150/2752 [13:01<15:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6094, 'grad_norm': 13.0625, 'learning_rate': 1.3211590148669586e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1200/2752 [13:29<14:30,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6106, 'grad_norm': 38.25, 'learning_rate': 1.2645357112230983e-05, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1250/2752 [13:57<14:03,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5883, 'grad_norm': 50.0, 'learning_rate': 1.2069846168027427e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1300/2752 [14:26<13:37,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6098, 'grad_norm': 11.6875, 'learning_rate': 1.1487075772256517e-05, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1350/2752 [14:54<13:24,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.605, 'grad_norm': 19.875, 'learning_rate': 1.0899089841769824e-05, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1376/2752 [17:20<12:47,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0, 'eval_accuracy': 0.7981730650536971, 'eval_macro_f1': 0.7864766375729122, 'eval_runtime': 130.7821, 'eval_samples_per_second': 61.943, 'eval_steps_per_second': 7.746, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1400/2752 [17:33<12:51,  1.75it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6098, 'grad_norm': 12.8125, 'learning_rate': 1.0307950585561705e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1450/2752 [18:01<12:09,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5901, 'grad_norm': 12.1875, 'learning_rate': 9.715731272103172e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1500/2752 [18:30<11:57,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6076, 'grad_norm': 7.875, 'learning_rate': 9.124508957887458e-06, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1550/2752 [18:58<11:19,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6047, 'grad_norm': 27.0, 'learning_rate': 8.536357202690115e-06, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1600/2752 [19:26<10:45,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5924, 'grad_norm': 14.125, 'learning_rate': 7.953338797092902e-06, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1650/2752 [19:54<10:21,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6021, 'grad_norm': 10.5, 'learning_rate': 7.377498527777887e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1700/2752 [20:23<09:58,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6121, 'grad_norm': 8.0, 'learning_rate': 6.810856005965558e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1750/2752 [20:51<09:25,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5923, 'grad_norm': 7.84375, 'learning_rate': 6.255398584149366e-06, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1800/2752 [21:19<08:59,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6177, 'grad_norm': 40.25, 'learning_rate': 5.713074385969457e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1850/2752 [21:47<08:30,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5701, 'grad_norm': 17.375, 'learning_rate': 5.185785473671484e-06, 'epoch': 2.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1900/2752 [22:16<07:56,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6067, 'grad_norm': 19.75, 'learning_rate': 4.675381177113837e-06, 'epoch': 2.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1950/2752 [22:44<07:39,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5931, 'grad_norm': 12.9375, 'learning_rate': 4.18365160772019e-06, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2000/2752 [23:12<07:05,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6162, 'grad_norm': 26.5, 'learning_rate': 3.7123213801253876e-06, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2050/2752 [23:40<06:34,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6223, 'grad_norm': 47.0, 'learning_rate': 3.2630435635344283e-06, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2064/2752 [25:59<06:25,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0, 'eval_accuracy': 0.7985433897049747, 'eval_macro_f1': 0.7867659779575287, 'eval_runtime': 130.1059, 'eval_samples_per_second': 62.265, 'eval_steps_per_second': 7.786, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2100/2752 [26:19<06:06,  1.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6246, 'grad_norm': 18.625, 'learning_rate': 2.837393884008608e-06, 'epoch': 3.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2150/2752 [26:47<05:43,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6023, 'grad_norm': 17.375, 'learning_rate': 2.4368651980127644e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2200/2752 [27:15<05:07,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6016, 'grad_norm': 23.875, 'learning_rate': 2.0628622566063063e-06, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2250/2752 [27:44<04:46,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5996, 'grad_norm': 8.25, 'learning_rate': 1.7166967786411493e-06, 'epoch': 3.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2300/2752 [28:12<04:17,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6068, 'grad_norm': 7.6875, 'learning_rate': 1.3995828502462072e-06, 'epoch': 3.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2350/2752 [28:40<03:47,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6098, 'grad_norm': 19.5, 'learning_rate': 1.1126326667334196e-06, 'epoch': 3.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2400/2752 [29:08<03:20,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5977, 'grad_norm': 19.25, 'learning_rate': 8.568526318595638e-07, 'epoch': 3.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2450/2752 [29:37<02:49,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5935, 'grad_norm': 15.6875, 'learning_rate': 6.33139828124657e-07, 'epoch': 3.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2500/2752 [30:05<02:23,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6226, 'grad_norm': 10.0625, 'learning_rate': 4.4227887048646335e-07, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2550/2752 [30:33<01:52,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5898, 'grad_norm': 10.5, 'learning_rate': 2.849391545259106e-07, 'epoch': 3.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2600/2752 [31:01<01:25,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6078, 'grad_norm': 44.25, 'learning_rate': 1.616725087147364e-07, 'epoch': 3.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2650/2752 [31:29<00:57,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5887, 'grad_norm': 46.0, 'learning_rate': 7.291125901946027e-08, 'epoch': 3.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2700/2752 [31:58<00:30,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6244, 'grad_norm': 21.5, 'learning_rate': 1.896671262955896e-08, 'epoch': 3.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2750/2752 [32:26<00:01,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5955, 'grad_norm': 25.5, 'learning_rate': 2.8066127798487274e-11, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2752/2752 [34:38<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0, 'eval_accuracy': 0.7985433897049747, 'eval_macro_f1': 0.7867659779575287, 'eval_runtime': 130.4547, 'eval_samples_per_second': 62.098, 'eval_steps_per_second': 7.765, 'epoch': 4.0}\n",
      "{'train_runtime': 2078.1359, 'train_samples_per_second': 42.38, 'train_steps_per_second': 1.324, 'train_loss': 0.6181378240242253, 'epoch': 4.0}\n",
      "\n",
      "[STAGE-2] train done. DEBUG_SMALL = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1013/1013 [02:09<00:00,  7.84it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1013/1013 [02:09<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SENTIMENT | validation | N=8101 ===\n",
      "Accuracy: 0.7985 | Macro-F1: 0.7868\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8425    0.8296    0.8360      2740\n",
      "     neutral     0.6092    0.7035    0.6530      2098\n",
      "    positive     0.9128    0.8336    0.8714      3263\n",
      "\n",
      "    accuracy                         0.7985      8101\n",
      "   macro avg     0.7881    0.7889    0.7868      8101\n",
      "weighted avg     0.8104    0.7985    0.8028      8101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:15<00:00,  7.91it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:15<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SARCASM | validation | N=955 ===\n",
      "Accuracy: 0.7717 | Macro-F1: 0.7642\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "non_sarcastic     0.7241    0.9098    0.8064       499\n",
      "    sarcastic     0.8628    0.6206    0.7219       456\n",
      "\n",
      "     accuracy                         0.7717       955\n",
      "    macro avg     0.7934    0.7652    0.7642       955\n",
      " weighted avg     0.7903    0.7717    0.7661       955\n",
      "\n",
      "\n",
      "== Saved Stage-2 artifacts ==\n",
      "Adapter dir: /media/veracrypt1/DS5500/project/stage2/stage2Result/output/adapter\n",
      "Sentiment head: /media/veracrypt1/DS5500/project/stage2/stage2Result/output/sent_head.pt\n",
      "Sarcasm head: /media/veracrypt1/DS5500/project/stage2/stage2Result/output/sarc_head.pt\n",
      "Sentiment results @ /media/veracrypt1/DS5500/project/stage2/stage2Result/sent_result\n",
      "Sarcasm   results @ /media/veracrypt1/DS5500/project/stage2/stage2Result/sarc_result\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Stage 2: Multi-task Fine-tuning (Sentiment + Sarcasm) [PRODUCTION]\n",
    "#   - Reuse Stage-1 LoRA + trained 3-way sentiment head\n",
    "#   - Add a new 2-way sarcasm head\n",
    "#   - Loss = CE_sarc + Î» * CE_sent  (masked by task)\n",
    "#   - Save artifacts to:\n",
    "#       /stage2/stage2Result/{sent_result, sarc_result, output}\n",
    "#   - All comments in English\n",
    "# =======================\n",
    "\n",
    "import os, json, random, numpy as np, pandas as pd, torch\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Basic env ----------\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"0\")\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "assert torch.cuda.is_available(), \"CUDA not found.\"\n",
    "\n",
    "SEED = 47\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ---------- Paths ----------\n",
    "PROJECT_ROOT    = Path(\"/media/veracrypt1/DS5500/project\")\n",
    "SENT_CSV        = PROJECT_ROOT / \"data\" / \"processed\" / \"sentiment_df.csv\"\n",
    "SARC_CSV        = PROJECT_ROOT / \"data\" / \"processed\" / \"sarcasm_df.csv\"\n",
    "\n",
    "# Reuse Stage-1 artifacts saved as required\n",
    "STAGE1_DIR      = PROJECT_ROOT / \"stage1\" / \"stage1Result\" / \"output\"\n",
    "STAGE1_ADAPTER  = STAGE1_DIR / \"adapter\"\n",
    "STAGE1_HEAD_PT  = STAGE1_DIR / \"sent_head.pt\"\n",
    "\n",
    "STAGE2_DIR      = PROJECT_ROOT / \"stage2\"\n",
    "STAGE2_WORK     = STAGE2_DIR / \"stage2_multitask\"     # Trainer working dir\n",
    "STAGE2_RESULT   = STAGE2_DIR / \"stage2Result\"         # Final results tree\n",
    "SENT_RESULT_DIR = STAGE2_RESULT / \"sent_result\"\n",
    "SARC_RESULT_DIR = STAGE2_RESULT / \"sarc_result\"\n",
    "OUTPUT_DIR      = STAGE2_RESULT / \"output\"\n",
    "for d in [STAGE2_WORK, SENT_RESULT_DIR, SARC_RESULT_DIR, OUTPUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "print(\"Device: cuda | GPU:\", torch.cuda.get_device_name(0),\n",
    "      \"| VRAM (GB):\", round(torch.cuda.get_device_properties(0).total_memory/1024**3,2))\n",
    "\n",
    "# ---------- PRODUCTION switch ----------\n",
    "DEBUG_SMALL = False           # <- production run (set True for quick debug)\n",
    "SARC_DEBUG_N = 1000           # used only if DEBUG_SMALL=True\n",
    "SENT_DEBUG_N = 1000\n",
    "VAL_DEBUG_N  = 300\n",
    "\n",
    "# ---------- Read & split ----------\n",
    "def norm_split(s):\n",
    "    s = str(s).strip().lower()\n",
    "    if s in {\"val\",\"valid\",\"validation\",\"dev\"}: return \"validation\"\n",
    "    if s in {\"train\",\"training\"}: return \"train\"\n",
    "    if s in {\"test\",\"testing\"}:  return \"test\"\n",
    "    return s\n",
    "\n",
    "# Sentiment\n",
    "sent_df_all = pd.read_csv(SENT_CSV)\n",
    "assert {\"text\",\"sentiment\",\"split\"}.issubset(sent_df_all.columns)\n",
    "sent_df_all[\"split_norm\"] = sent_df_all[\"split\"].map(norm_split)\n",
    "sent_train_df = sent_df_all[sent_df_all[\"split_norm\"]==\"train\"][[\"text\",\"sentiment\"]].rename(\n",
    "    columns={\"sentiment\":\"label_sent\"}).reset_index(drop=True)\n",
    "sent_val_df   = sent_df_all[sent_df_all[\"split_norm\"]==\"validation\"][[\"text\",\"sentiment\"]].rename(\n",
    "    columns={\"sentiment\":\"label_sent\"}).reset_index(drop=True)\n",
    "sent_train_df[\"label_sent\"] = sent_train_df[\"label_sent\"].astype(int)\n",
    "sent_val_df[\"label_sent\"]   = sent_val_df[\"label_sent\"].astype(int)\n",
    "\n",
    "# Sarcasm\n",
    "sarc_df_all = pd.read_csv(SARC_CSV)\n",
    "assert \"text\" in sarc_df_all.columns and \"split\" in sarc_df_all.columns\n",
    "if \"sarcasm\" in sarc_df_all.columns:\n",
    "    sarc_df_all = sarc_df_all.rename(columns={\"sarcasm\":\"label_sarc\"})\n",
    "elif \"label\" in sarc_df_all.columns:\n",
    "    sarc_df_all = sarc_df_all.rename(columns={\"label\":\"label_sarc\"})\n",
    "else:\n",
    "    raise ValueError(\"Sarcasm csv must have 'sarcasm' or 'label' column.\")\n",
    "sarc_df_all[\"split_norm\"] = sarc_df_all[\"split\"].map(norm_split)\n",
    "sarc_train_df = sarc_df_all[sarc_df_all[\"split_norm\"]==\"train\"][[\"text\",\"label_sarc\"]].reset_index(drop=True)\n",
    "sarc_val_df   = sarc_df_all[sarc_df_all[\"split_norm\"]==\"validation\"][[\"text\",\"label_sarc\"]].reset_index(drop=True)\n",
    "sarc_train_df[\"label_sarc\"] = sarc_train_df[\"label_sarc\"].astype(int)\n",
    "sarc_val_df[\"label_sarc\"]   = sarc_val_df[\"label_sarc\"].astype(int)\n",
    "\n",
    "# ---------- Build replay (sentiment) + sarcasm multi-task train ----------\n",
    "LAMBDA_SENT  = 0.5     # loss = CE_sarc + Î» * CE_sent\n",
    "REPLAY_RATIO = 1.0     # sentiment replay size relative to sarcasm train size\n",
    "\n",
    "replay_target = int(REPLAY_RATIO * len(sarc_train_df))\n",
    "g = sent_train_df.groupby(\"label_sent\", group_keys=False)\n",
    "per_cls = max(1, replay_target // max(1, g.size().shape[0]))\n",
    "# stratified-ish balanced sampling\n",
    "replay_sent_df = g.apply(lambda x: x.sample(n=min(per_cls, len(x)), random_state=SEED),\n",
    "                         include_groups=False).reset_index(drop=True)\n",
    "if len(replay_sent_df) < replay_target and len(sent_train_df) > len(replay_sent_df):\n",
    "    need = replay_target - len(replay_sent_df)\n",
    "    replay_sent_df = pd.concat([replay_sent_df, sent_train_df.sample(n=need, random_state=SEED)],\n",
    "                               ignore_index=True)\n",
    "replay_sent_df = replay_sent_df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Optional debug subsampling\n",
    "if DEBUG_SMALL:\n",
    "    sarc_train_df  = sarc_train_df.sample(n=min(SARC_DEBUG_N, len(sarc_train_df)),\n",
    "                                          random_state=SEED).reset_index(drop=True)\n",
    "    replay_sent_df = replay_sent_df.sample(n=min(SENT_DEBUG_N, len(replay_sent_df)),\n",
    "                                           random_state=SEED).reset_index(drop=True)\n",
    "    sent_val_df    = sent_val_df.sample(n=min(VAL_DEBUG_N, len(sent_val_df)),\n",
    "                                        random_state=SEED).reset_index(drop=True)\n",
    "    sarc_val_df    = sarc_val_df.sample(n=min(VAL_DEBUG_N, len(sarc_val_df)),\n",
    "                                        random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Merge into multi-task training dataframe\n",
    "mt_train_df = pd.DataFrame({\n",
    "    \"text\": pd.concat([sarc_train_df[\"text\"], replay_sent_df[\"text\"]], ignore_index=True),\n",
    "})\n",
    "mt_train_df[\"label_sarc\"] = pd.concat(\n",
    "    [sarc_train_df[\"label_sarc\"], pd.Series([-100]*len(replay_sent_df))], ignore_index=True)\n",
    "mt_train_df[\"label_sent\"] = pd.concat(\n",
    "    [pd.Series([-100]*len(sarc_train_df)), replay_sent_df[\"label_sent\"]], ignore_index=True)\n",
    "mt_train_df[\"task_id\"]    = (mt_train_df[\"label_sarc\"] != -100).astype(int)  # 1=sarc, 0=sent\n",
    "\n",
    "both_mask = (mt_train_df[\"label_sent\"]==-100) & (mt_train_df[\"label_sarc\"]==-100)\n",
    "assert not both_mask.any(), f\"Bad rows with both labels -100: {both_mask.sum()}\"\n",
    "\n",
    "# Eval sets: each single-task with 'labels' column for HF Trainer\n",
    "sent_val_df = sent_val_df[[\"text\",\"label_sent\"]].reset_index(drop=True)\n",
    "sarc_val_df = sarc_val_df[[\"text\",\"label_sarc\"]].reset_index(drop=True)\n",
    "\n",
    "print(f\"[REPLAY] sentiment sampled = {len(replay_sent_df)}\")\n",
    "print(f\"[MT-TRAIN] total={len(mt_train_df)} | frac sarcasm={mt_train_df['task_id'].mean():.3f}\")\n",
    "print(f\"[SENT] train_replay={len(replay_sent_df)}, val={len(sent_val_df)}\")\n",
    "print(f\"[SARC] train={len(sarc_train_df)}, val={len(sarc_val_df)}\")\n",
    "\n",
    "# ---------- Tokenization ----------\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tok.padding_side = \"right\"; tok.truncation_side = \"right\"\n",
    "if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "def tok_mt_train(batch):\n",
    "    enc = tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "    def nz(x):  # NaN -> -100\n",
    "        return -100 if (x is None or (isinstance(x, float) and np.isnan(x))) else int(x)\n",
    "    enc[\"labels_sent\"] = [nz(x) for x in batch[\"label_sent\"]]\n",
    "    enc[\"labels_sarc\"] = [nz(x) for x in batch[\"label_sarc\"]]\n",
    "    enc[\"task_id\"]     = [int(x) for x in batch[\"task_id\"]]\n",
    "    return enc\n",
    "\n",
    "def tok_eval_sent(batch):\n",
    "    enc = tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "    enc[\"labels\"]  = [int(x) for x in batch[\"label_sent\"]]\n",
    "    enc[\"task_id\"] = [0]*len(batch[\"label_sent\"])     # whole batch = sentiment\n",
    "    return enc\n",
    "\n",
    "def tok_eval_sarc(batch):\n",
    "    enc = tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "    enc[\"labels\"]  = [int(x) for x in batch[\"label_sarc\"]]\n",
    "    enc[\"task_id\"] = [1]*len(batch[\"label_sarc\"])     # whole batch = sarcasm\n",
    "    return enc\n",
    "\n",
    "mt_train_tok = Dataset.from_pandas(mt_train_df).map(\n",
    "    tok_mt_train, batched=True, remove_columns=list(mt_train_df.columns)\n",
    ").with_format(\"torch\")\n",
    "sent_val_tok  = Dataset.from_pandas(sent_val_df).map(\n",
    "    tok_eval_sent, batched=True, remove_columns=list(sent_val_df.columns)\n",
    ").with_format(\"torch\")\n",
    "sarc_val_tok  = Dataset.from_pandas(sarc_val_df).map(\n",
    "    tok_eval_sarc, batched=True, remove_columns=list(sarc_val_df.columns)\n",
    ").with_format(\"torch\")\n",
    "\n",
    "# ---------- Load Stage-1 base + LoRA + trained 3-way head ----------\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "from torch import nn\n",
    "\n",
    "use_bf16    = torch.cuda.is_bf16_supported()\n",
    "torch_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "base_seqcls = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# attach Stage-1 adapter\n",
    "base_seqcls = PeftModel.from_pretrained(base_seqcls, STAGE1_ADAPTER)\n",
    "\n",
    "# load Stage-1 trained 3-way head\n",
    "assert STAGE1_HEAD_PT.exists(), f\"Missing Stage-1 head: {STAGE1_HEAD_PT}\"\n",
    "head_state = torch.load(STAGE1_HEAD_PT, map_location=\"cpu\")\n",
    "base_seqcls.score.load_state_dict(head_state)\n",
    "\n",
    "# ---------- Multi-task wrapper ----------\n",
    "class MultiTaskSentSarc(nn.Module):\n",
    "    \"\"\"\n",
    "    Reuse Stage-1 peft_seqcls (with trained 3-way head) + add a new 2-way sarcasm head.\n",
    "    Masked loss: CE over available labels, loss = CE_sarc + lambda * CE_sent.\n",
    "    For evaluation:\n",
    "      - If the batch is all sarcasm (task_id=1), return 2-way logits\n",
    "      - Otherwise return 3-way logits (default)\n",
    "    \"\"\"\n",
    "    def __init__(self, peft_seqcls_model: nn.Module, pad_token_id: int, lambda_sent: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.peft_seqcls = peft_seqcls_model\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.lambda_sent = lambda_sent\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        # dtype/device alignment for the new sarcasm head\n",
    "        sample_param = next(self.peft_seqcls.parameters())\n",
    "        hidden = self.peft_seqcls.config.hidden_size\n",
    "        self.sarc_head = nn.Linear(hidden, 2, device=sample_param.device, dtype=sample_param.dtype)\n",
    "        # ensure hidden states are returned\n",
    "        self.peft_seqcls.config.output_hidden_states = True\n",
    "\n",
    "    def _pool_last_nonpad(self, hidden_states, attention_mask):\n",
    "        if attention_mask is None:\n",
    "            return hidden_states[:, -1, :]\n",
    "        idx = attention_mask.long().sum(dim=1) - 1\n",
    "        idx = torch.clamp(idx, min=0)\n",
    "        return hidden_states[torch.arange(hidden_states.size(0), device=hidden_states.device), idx]\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,\n",
    "                labels_sent=None, labels_sarc=None, task_id=None, **kwargs):\n",
    "        # unwrap backbone (transformer body)\n",
    "        if hasattr(self.peft_seqcls, \"get_base_model\"):\n",
    "            backbone = self.peft_seqcls.get_base_model().model\n",
    "        else:\n",
    "            backbone = getattr(getattr(self.peft_seqcls, \"base_model\", self.peft_seqcls), \"model\", self.peft_seqcls)\n",
    "\n",
    "        out = backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        hidden = out.last_hidden_state if hasattr(out, \"last_hidden_state\") else out.hidden_states[-1]\n",
    "        pooled = self._pool_last_nonpad(hidden, attention_mask)\n",
    "\n",
    "        logits_sent = self.peft_seqcls.score(pooled)   # [B,3]\n",
    "        logits_sarc = self.sarc_head(pooled)           # [B,2]\n",
    "\n",
    "        # masked multi-task loss\n",
    "        loss = None\n",
    "        if labels_sent is not None and (labels_sent != -100).any():\n",
    "            m = (labels_sent != -100)\n",
    "            loss_sent = self.ce(logits_sent[m], labels_sent[m]).mean()\n",
    "            loss = (self.lambda_sent * loss_sent) if loss is None else (loss + self.lambda_sent * loss_sent)\n",
    "        if labels_sarc is not None and (labels_sarc != -100).any():\n",
    "            m = (labels_sarc != -100)\n",
    "            loss_sarc = self.ce(logits_sarc[m], labels_sarc[m]).mean()\n",
    "            loss = loss_sarc if loss is None else (loss + loss_sarc)\n",
    "        if loss is None:\n",
    "            loss = (logits_sent.mean()*0) + (logits_sarc.mean()*0)\n",
    "\n",
    "        # return one logits tensor for HF predict/eval:\n",
    "        logits = logits_sent\n",
    "        if task_id is not None and torch.all(task_id == 1):\n",
    "            logits = logits_sarc\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits,\n",
    "                \"logits_sent\": logits_sent, \"logits_sarc\": logits_sarc}\n",
    "\n",
    "mt_model = MultiTaskSentSarc(base_seqcls, pad_token_id=tok.pad_token_id, lambda_sent=LAMBDA_SENT).to(\"cuda\")\n",
    "\n",
    "# ---------- Custom Trainer ----------\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "class MTTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # pop custom labels; keep 'labels' (eval only)\n",
    "        labels_sent = inputs.pop(\"labels_sent\", None)\n",
    "        labels_sarc = inputs.pop(\"labels_sarc\", None)\n",
    "        outputs = model(**inputs, labels_sent=labels_sent, labels_sarc=labels_sarc)\n",
    "        loss = outputs.get(\"loss\", None)\n",
    "        if loss is None:\n",
    "            ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "            ls = lr = 0.0; count = 0\n",
    "            if labels_sarc is not None and (labels_sarc != -100).any():\n",
    "                m = (labels_sarc != -100)\n",
    "                lr = ce(outputs[\"logits_sarc\"][m], labels_sarc[m]).mean()\n",
    "                loss = lr; count += 1\n",
    "            if labels_sent is not None and (labels_sent != -100).any():\n",
    "                m = (labels_sent != -100)\n",
    "                ls = ce(outputs[\"logits_sent\"][m], labels_sent[m]).mean()\n",
    "                loss = (loss if count else 0) + getattr(model, \"lambda_sent\", 0.5) * ls\n",
    "            if loss is None:\n",
    "                loss = outputs[\"logits\"].mean()*0\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    # Ensure metrics always see a single (N, C) logits tensor\n",
    "    def preprocess_logits_for_metrics(self, logits, labels):\n",
    "        if isinstance(logits, (tuple, list)):\n",
    "            return logits[0]\n",
    "        return logits\n",
    "\n",
    "def basic_metrics(p):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, (tuple,list)) else p.predictions\n",
    "    pred = np.argmax(preds, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(p.label_ids, pred)),\n",
    "        \"macro_f1\": float(f1_score(p.label_ids, pred, average=\"macro\"))\n",
    "    }\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM = 4\n",
    "\n",
    "mt_args = TrainingArguments(\n",
    "    output_dir=str(STAGE2_WORK),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100 if not DEBUG_SMALL else 50,\n",
    "    num_train_epochs=4 if not DEBUG_SMALL else 1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    logging_steps=50 if not DEBUG_SMALL else 20,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=8,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    # IMPORTANT: keep ONLY \"labels\" here; do not include labels_sent/labels_sarc\n",
    "    label_names=[\"labels\"],\n",
    "    save_safetensors=False,\n",
    ")\n",
    "\n",
    "mt_trainer = MTTrainer(\n",
    "    model=mt_model,\n",
    "    args=mt_args,\n",
    "    train_dataset=mt_train_tok,\n",
    "    eval_dataset=sent_val_tok,     # default eval = sentiment; we'll evaluate both below\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=basic_metrics,\n",
    ")\n",
    "\n",
    "# ---------- Train ----------\n",
    "train_result = mt_trainer.train()\n",
    "print(\"\\n[STAGE-2] train done. DEBUG_SMALL =\", DEBUG_SMALL)\n",
    "\n",
    "# ---------- Eval helper (robust to HF quirks) ----------\n",
    "def eval_and_save(name, eval_ds, out_dir, n_classes, id2label):\n",
    "    \"\"\"\n",
    "    - Uses trainer.evaluate for loss/metrics.\n",
    "    - Uses trainer.predict for per-sample outputs; gold labels read from eval_ds['labels'].\n",
    "    - Saves confusion matrix / classification report / per-sample CSV / raw logits.npy.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) evaluate (loss + compute_metrics)\n",
    "    r = mt_trainer.evaluate(eval_dataset=eval_ds, metric_key_prefix=name)\n",
    "    with open(out_dir / \"metrics_eval.json\", \"w\") as f:\n",
    "        json.dump({k: float(v) if isinstance(v, (int,float,np.floating)) else v for k,v in r.items()}, f, indent=2)\n",
    "\n",
    "    # 2) predict (logits)\n",
    "    pred = mt_trainer.predict(eval_ds)\n",
    "    logits = pred.predictions[0] if isinstance(pred.predictions, (tuple, list)) else pred.predictions\n",
    "    y_pred = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # read gold labels directly from dataset\n",
    "    y_true = np.array(eval_ds[\"labels\"], dtype=int)\n",
    "\n",
    "    # aggregate metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    cm  = confusion_matrix(y_true, y_pred, labels=list(range(n_classes)))\n",
    "\n",
    "    # save confusion matrix\n",
    "    cm_df = pd.DataFrame(cm, index=[id2label[i] for i in range(n_classes)],\n",
    "                            columns=[id2label[i] for i in range(n_classes)])\n",
    "    cm_df.to_csv(out_dir / \"confusion_matrix.csv\", index=True)\n",
    "\n",
    "    # save classification report\n",
    "    report_str = classification_report(\n",
    "        y_true, y_pred, target_names=[id2label[i] for i in range(n_classes)], digits=4\n",
    "    )\n",
    "    with open(out_dir / \"classification_report.txt\", \"w\") as f:\n",
    "        f.write(report_str)\n",
    "\n",
    "    # save per-sample predictions with text\n",
    "    if name == \"sentiment\":\n",
    "        src_df = sent_val_df.copy()\n",
    "    else:\n",
    "        src_df = sarc_val_df.copy()\n",
    "\n",
    "    per_sample = pd.DataFrame({\n",
    "        \"pred_label\": y_pred,\n",
    "        \"pred_name\":  [id2label[int(i)] for i in y_pred],\n",
    "        \"true_label\": y_true,\n",
    "        \"true_name\":  [id2label[int(i)] for i in y_true],\n",
    "    })\n",
    "    L = min(len(src_df), len(per_sample))\n",
    "    src_df = src_df.iloc[:L].reset_index(drop=True)\n",
    "    per_sample = per_sample.iloc[:L].reset_index(drop=True)\n",
    "    merged = pd.concat([src_df, per_sample], axis=1)\n",
    "    merged.to_csv(out_dir / \"val_predictions_with_text.csv\", index=False)\n",
    "\n",
    "    # save raw logits\n",
    "    np.save(out_dir / \"val_logits.npy\", logits)\n",
    "\n",
    "    # console summary\n",
    "    print(f\"\\n=== {name.upper()} | validation | N={len(y_true)} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Macro-F1: {mf1:.4f}\")\n",
    "    print(report_str)\n",
    "\n",
    "# ---------- Evaluate both tasks ----------\n",
    "id2label_sent = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\n",
    "id2label_sarc = {0:\"non_sarcastic\", 1:\"sarcastic\"}\n",
    "\n",
    "eval_and_save(\"sentiment\", sent_val_tok, SENT_RESULT_DIR, n_classes=3, id2label=id2label_sent)\n",
    "eval_and_save(\"sarcasm\",   sarc_val_tok, SARC_RESULT_DIR, n_classes=2, id2label=id2label_sarc)\n",
    "\n",
    "# ---------- Save adapters + heads + configs ----------\n",
    "# Save peft adapter (LoRA) to Stage-2 result tree (snapshot)\n",
    "ADAPTER_DIR = OUTPUT_DIR / \"adapter\"\n",
    "ADAPTER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "mt_model.peft_seqcls.save_pretrained(ADAPTER_DIR)   # LoRA adapter + peft config\n",
    "tok.save_pretrained(ADAPTER_DIR)\n",
    "\n",
    "# Save both heads (.pt)\n",
    "torch.save(mt_model.peft_seqcls.score.state_dict(), OUTPUT_DIR / \"sent_head.pt\")\n",
    "torch.save(mt_model.sarc_head.state_dict(),        OUTPUT_DIR / \"sarc_head.pt\")\n",
    "\n",
    "# Save run meta\n",
    "with open(OUTPUT_DIR / \"run_meta_stage2.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"seed\": SEED,\n",
    "        \"debug_mode\": DEBUG_SMALL,\n",
    "        \"lambda_sent\": LAMBDA_SENT,\n",
    "        \"replay_ratio\": REPLAY_RATIO,\n",
    "        \"train_sizes\": {\n",
    "            \"sarc_train\": int(len(sarc_train_df)),\n",
    "            \"sent_replay_train\": int(len(replay_sent_df)),\n",
    "            \"mt_train_total\": int(len(mt_train_df)),\n",
    "        },\n",
    "        \"val_sizes\": {\n",
    "            \"sent_val\": int(len(sent_val_df)),\n",
    "            \"sarc_val\": int(len(sarc_val_df)),\n",
    "        },\n",
    "        \"id2label_sent\": id2label_sent,\n",
    "        \"id2label_sarc\": id2label_sarc,\n",
    "        \"stage1_adapter_dir\": str(STAGE1_ADAPTER),\n",
    "        \"stage1_head_pt\": str(STAGE1_HEAD_PT)\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n== Saved Stage-2 artifacts ==\")\n",
    "print(\"Adapter dir:\", ADAPTER_DIR)\n",
    "print(\"Sentiment head:\", OUTPUT_DIR / \"sent_head.pt\")\n",
    "print(\"Sarcasm head:\",   OUTPUT_DIR / \"sarc_head.pt\")\n",
    "print(\"Sentiment results @\", SENT_RESULT_DIR)\n",
    "print(\"Sarcasm   results @\", SARC_RESULT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc67ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4ae13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2a634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce35f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e5405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea5151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58032755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa6d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaFT_env (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
