{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03ce9ef",
   "metadata": {},
   "source": [
    "## Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f476921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/penglishao/anaconda3/envs/llama-8B/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"YourToken\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6afe6",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a964572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SENTIMENT MERGED =====\n",
      "Split counts:\n",
      "split\n",
      "test           19494\n",
      "train         254159\n",
      "validation      8101\n",
      "\n",
      "By source × split:\n",
      "split                       test   train  validation\n",
      "source                                              \n",
      "sentiment_df_TE.csv        12284   45615        2000\n",
      "sentiment_marc_mapped.csv   5000  200000        5000\n",
      "sentiment_sst5_mapped.csv   2210    8544        1101\n",
      "\n",
      "Total rows: 281754 (columns: ['sentiment', 'text', 'split', 'source'])\n",
      "\n",
      "===== SARCASM MERGED =====\n",
      "Split counts:\n",
      "split\n",
      "test           2414\n",
      "train         11009\n",
      "validation      955\n",
      "\n",
      "By source × split:\n",
      "split                        test  train  validation\n",
      "source                                              \n",
      "isarcasmeval_merged.csv      1630   4326           0\n",
      "sarcasm_df_TE.csv             784   2862         955\n",
      "semeval2018_irony_train.csv     0   3821           0\n",
      "\n",
      "Total rows: 14378 (columns: ['sarcasm', 'text', 'split', 'source'])\n",
      "\n",
      "[OK] Saved:\n",
      "  /Users/penglishao/Desktop/DS5500/project/data/processed/sentiment_df.csv\n",
      "  /Users/penglishao/Desktop/DS5500/project/data/processed/sarcasm_df.csv\n"
     ]
    }
   ],
   "source": [
    "# 项目根目录（如需在别处运行，改这里）\n",
    "PROJECT_ROOT = \"/Users/penglishao/Desktop/DS5500/project\"\n",
    "PROC_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "SENT_DIR = os.path.join(PROC_DIR, \"sentiment_data\")\n",
    "SARC_DIR = os.path.join(PROC_DIR, \"sarcasm_data\")\n",
    "\n",
    "def normalize_split(s):\n",
    "    s = str(s).strip().lower()\n",
    "    if s in {\"val\", \"valid\", \"validation\", \"dev\"}:\n",
    "        return \"validation\"\n",
    "    if s in {\"train\", \"training\"}:\n",
    "        return \"train\"\n",
    "    if s in {\"test\", \"testing\"}:\n",
    "        return \"test\"\n",
    "    return s  # 其余保持原样，方便你发现异常取值\n",
    "\n",
    "def load_and_merge(root, kind):\n",
    "    \"\"\"\n",
    "    kind=\"sentiment\" 需要列: text, sentiment, split\n",
    "    kind=\"sarcasm\"   需要列: text, sarcasm,  split\n",
    "    \"\"\"\n",
    "    need = {\"text\", kind, \"split\"}\n",
    "    frames = []\n",
    "    for p in sorted(glob.glob(os.path.join(root, \"*.csv\"))):\n",
    "        df = pd.read_csv(p)\n",
    "        missing = need - set(df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"File {os.path.basename(p)} 缺少列: {missing}. 现有列: {list(df.columns)}\")\n",
    "        sub = df[list(need)].copy()\n",
    "        sub[\"split\"] = sub[\"split\"].map(normalize_split)\n",
    "        sub[\"source\"] = os.path.basename(p)\n",
    "        frames.append(sub)\n",
    "    if not frames:\n",
    "        raise RuntimeError(f\"目录为空: {root}\")\n",
    "    merged = pd.concat(frames, ignore_index=True)\n",
    "    return merged\n",
    "\n",
    "def describe(df, label_col, title):\n",
    "    print(f\"\\n===== {title} =====\")\n",
    "    # 总体 split 统计\n",
    "    split_counts = df[\"split\"].value_counts(dropna=False).sort_index()\n",
    "    print(\"Split counts:\")\n",
    "    print(split_counts.to_string())\n",
    "    # 各数据源 × split\n",
    "    ctab = df.pivot_table(index=\"source\", columns=\"split\", values=label_col, aggfunc=\"count\", fill_value=0)\n",
    "    print(\"\\nBy source × split:\")\n",
    "    print(ctab.to_string())\n",
    "    # 简要样本量\n",
    "    print(f\"\\nTotal rows: {len(df)} (columns: {list(df.columns)})\")\n",
    "\n",
    "def main():\n",
    "    os.makedirs(PROC_DIR, exist_ok=True)\n",
    "\n",
    "    # 1) 合并情感\n",
    "    sent_df = load_and_merge(SENT_DIR, kind=\"sentiment\")\n",
    "    out_sent_csv = os.path.join(PROC_DIR, \"sentiment_df.csv\")\n",
    "    sent_df.to_csv(out_sent_csv, index=False, encoding=\"utf-8\")\n",
    "    describe(sent_df, label_col=\"sentiment\", title=\"SENTIMENT MERGED\")\n",
    "\n",
    "    # 2) 合并讽刺\n",
    "    sarc_df = load_and_merge(SARC_DIR, kind=\"sarcasm\")\n",
    "    out_sarc_csv = os.path.join(PROC_DIR, \"sarcasm_df.csv\")\n",
    "    sarc_df.to_csv(out_sarc_csv, index=False, encoding=\"utf-8\")\n",
    "    describe(sarc_df, label_col=\"sarcasm\", title=\"SARCASM MERGED\")\n",
    "\n",
    "    print(f\"\\n[OK] Saved:\\n  {out_sent_csv}\\n  {out_sarc_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd61c25",
   "metadata": {},
   "source": [
    "## Sentiment and Sarvasm Baseline Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a35800d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment CSV: ../data/processed/sentiment_df.csv\n",
      "Sarcasm  CSV: ../data/processed/sarcasm_df.csv\n",
      "Out dir     : runs/llama31_zeroshot_eval\n",
      "Device=cpu, dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Llama 3.1 8B zero-shot eval on validation split for sentiment & sarcasm\n",
    "\n",
    "# %%\n",
    "import os, json, re, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --------- Paths (auto-resolve for running from baseline/ or project/) ----------\n",
    "def resolve_path(*candidates):\n",
    "    for p in candidates:\n",
    "        if p and os.path.exists(p):\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"None of these paths exist:\\n{candidates}\")\n",
    "\n",
    "SENT_CSV = resolve_path(\n",
    "    \"../data/processed/sentiment_df.csv\",   # running from baseline/\n",
    "    \"data/processed/sentiment_df.csv\"       # running from project/\n",
    ")\n",
    "SARC_CSV = resolve_path(\n",
    "    \"../data/processed/sarcasm_df.csv\",\n",
    "    \"data/processed/sarcasm_df.csv\"\n",
    ")\n",
    "OUT_DIR  = \"runs/llama31_zeroshot_eval\"    # under baseline/\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print(\"Sentiment CSV:\", SENT_CSV)\n",
    "print(\"Sarcasm  CSV:\", SARC_CSV)\n",
    "print(\"Out dir     :\", OUT_DIR)\n",
    "\n",
    "# --------- Device/Dtype ----------\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    DTYPE  = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     DEVICE = \"mps\"\n",
    "#     DTYPE  = torch.float16\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    DTYPE  = torch.float32\n",
    "print(f\"Device={DEVICE}, dtype={DTYPE}\")\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "SENTIMENT_LABELS = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "SARCASM_LABELS   = {0: \"non-sarcastic\", 1: \"sarcastic\"}\n",
    "\n",
    "def normalize_split(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    t = str(s).strip().lower()\n",
    "    if t in {\"val\",\"valid\",\"validation\",\"dev\"}: return \"validation\"\n",
    "    if t in {\"train\",\"training\"}:               return \"train\"\n",
    "    if t in {\"test\",\"testing\"}:                 return \"test\"\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc493ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def build_chat_prompt_sentiment(text: str):\n",
    "    system = (\n",
    "        \"You are a precise sentiment classifier. \"\n",
    "        \"Classify the user's message into one of three classes: \"\n",
    "        \"0=negative, 1=neutral, 2=positive. \"\n",
    "        \"Return exactly one digit (0 or 1 or 2). Output nothing else.\"\n",
    "    )\n",
    "    user = f\"Message:\\n{text}\\n\\nReturn only one digit (0/1/2).\"\n",
    "    return system, user\n",
    "\n",
    "def build_chat_prompt_sarcasm(text: str):\n",
    "    system = (\n",
    "        \"You are a precise sarcasm/irony detector. \"\n",
    "        'Decide if the user\\'s message is sarcastic. '\n",
    "        \"Use 0=non-sarcastic, 1=sarcastic. \"\n",
    "        \"Return exactly one digit (0 or 1). Output nothing else.\"\n",
    "    )\n",
    "    user = f\"Message:\\n{text}\\n\\nReturn only one digit (0/1).\"\n",
    "    return system, user\n",
    "\n",
    "_digit012 = re.compile(r\"([012])\")\n",
    "_digit01  = re.compile(r\"([01])\")\n",
    "\n",
    "def parse_digit_012(s: str) -> int | None:\n",
    "    if s is None: return None\n",
    "    t = s.strip()\n",
    "    if t in {\"0\",\"1\",\"2\"}: return int(t)\n",
    "    m = _digit012.search(t);  0\n",
    "    if m: return int(m.group(1))\n",
    "    lo = t.lower()\n",
    "    if \"negative\" in lo: return 0\n",
    "    if \"neutral\"  in lo: return 1\n",
    "    if \"positive\" in lo: return 2\n",
    "    return None\n",
    "\n",
    "def parse_digit_01(s: str) -> int | None:\n",
    "    if s is None: return None\n",
    "    t = s.strip()\n",
    "    if t in {\"0\",\"1\"}: return int(t)\n",
    "    m = _digit01.search(t)\n",
    "    if m: return int(m.group(1))\n",
    "    lo = t.lower()\n",
    "    if \"non-sarcastic\" in lo or \"non sarcastic\" in lo: return 0\n",
    "    if \"sarcastic\" in lo: return 1\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7024a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_model(model_id: str = MODEL_ID):\n",
    "    print(f\"[INFO] Loading: {model_id}\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=DTYPE,           # 新版用 dtype\n",
    "        device_map=\"auto\",     # 自动放置；MPS/CPU/单卡CUDA都可\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok, model\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_batch(tokenizer, model, systems, users, max_new_tokens=2):\n",
    "    prompts = []\n",
    "    for sys, usr in zip(systems, users):\n",
    "        messages = [{\"role\":\"system\",\"content\":sys}, {\"role\":\"user\",\"content\":usr}]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    ).to(model.device)\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,                  # 贪心；稳定输出单字符\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    gen = out[:, inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af239d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def run_eval_on_validation(df: pd.DataFrame, task: str, tokenizer, model, batch_size=8):\n",
    "    \"\"\"\n",
    "    只评测 validation；若没有 validation，则返回空结果并提示。\n",
    "    列要求：\n",
    "      sentiment: text,sentiment,split\n",
    "      sarcasm:   text,sarcasm,split\n",
    "    \"\"\"\n",
    "    assert task in {\"sentiment\",\"sarcasm\"}\n",
    "    label_col = \"sentiment\" if task==\"sentiment\" else \"sarcasm\"\n",
    "    id2name   = SENTIMENT_LABELS if task==\"sentiment\" else SARCASM_LABELS\n",
    "    parser    = parse_digit_012 if task==\"sentiment\" else parse_digit_01\n",
    "    build     = build_chat_prompt_sentiment if task==\"sentiment\" else build_chat_prompt_sarcasm\n",
    "\n",
    "    # 只取 validation\n",
    "    sub = df.copy()\n",
    "    sub[\"split_norm\"] = sub[\"split\"].map(normalize_split)\n",
    "    sub = sub[sub[\"split_norm\"]==\"validation\"][[\"text\", label_col, \"split\"]].reset_index(drop=True)\n",
    "\n",
    "    if sub.empty:\n",
    "        print(f\"[Skip] No validation rows for task={task}.\")\n",
    "        return None\n",
    "\n",
    "    preds, gts = [], []\n",
    "    systems, users, idx_buf = [], [], []\n",
    "    pbar = tqdm(total=len(sub), desc=f\"{task.capitalize()} (validation)\", unit=\"sample\")\n",
    "\n",
    "    for i, row in sub.iterrows():\n",
    "        sys, usr = build(row[\"text\"])\n",
    "        systems.append(sys); users.append(usr); idx_buf.append(i)\n",
    "\n",
    "        if len(systems) >= batch_size:\n",
    "            outs = generate_batch(tokenizer, model, systems, users)\n",
    "            for j, out in enumerate(outs):\n",
    "                p = parser(out)\n",
    "                if p is None: p = 1 if task==\"sentiment\" else 0\n",
    "                preds.append(p); gts.append(int(sub.iloc[idx_buf[j]][label_col]))\n",
    "                pbar.update(1)\n",
    "            systems, users, idx_buf = [], [], []\n",
    "\n",
    "    if systems:\n",
    "        outs = generate_batch(tokenizer, model, systems, users)\n",
    "        for j, out in enumerate(outs):\n",
    "            p = parser(out)\n",
    "            if p is None: p = 1 if task==\"sentiment\" else 0\n",
    "            preds.append(p); gts.append(int(sub.iloc[idx_buf[j]][label_col]))\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    preds = np.array(preds); gts = np.array(gts)\n",
    "    labels = sorted(set(gts) | set(preds))\n",
    "    cm = confusion_matrix(gts, preds, labels=labels)\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    macro_f1 = f1_score(gts, preds, average=\"macro\")\n",
    "    rep = classification_report(gts, preds, target_names=[id2name[i] for i in labels], digits=4)\n",
    "\n",
    "    # >>> 把 NumPy 类型都转成纯 Python 可 JSON 化的类型\n",
    "    labels_list = [int(x) for x in labels]\n",
    "    cm_list = cm.astype(int).tolist()\n",
    "\n",
    "    return {\n",
    "        \"labels\": labels_list,           # 纯 Python int\n",
    "        \"confusion_matrix\": cm_list,     # 纯 Python list[list[int]]\n",
    "        \"accuracy\": float(acc),          # 纯 Python float\n",
    "        \"macro_f1\": float(macro_f1),     # 纯 Python float\n",
    "        \"report\": str(rep),              # 字符串\n",
    "        \"n\": int(len(sub)),              # 纯 Python int\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9f6b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment split counts: {'train': 254159, 'test': 19494, 'validation': 8101}\n",
      "Sarcasm  split counts: {'train': 11009, 'test': 2414, 'validation': 955}\n",
      "Sentiment validation size: 8101\n",
      "Sarcasm  validation size: 955\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "sent_df = pd.read_csv(SENT_CSV)\n",
    "sarc_df = pd.read_csv(SARC_CSV)\n",
    "# 基础列检查\n",
    "assert {\"text\",\"sentiment\",\"split\"}.issubset(sent_df.columns), f\"Sentiment cols wrong: {sent_df.columns}\"\n",
    "assert {\"text\",\"sarcasm\",\"split\"}.issubset(sarc_df.columns),  f\"Sarcasm cols wrong: {sarc_df.columns}\"\n",
    "\n",
    "# 查看各自 validation 数量\n",
    "def count_val(df):\n",
    "    x = df[\"split\"].map(normalize_split).value_counts()\n",
    "    return int(x.get(\"validation\", 0)), x.to_dict()\n",
    "\n",
    "sent_val_n, sent_split_counts = count_val(sent_df)\n",
    "sarc_val_n, sarc_split_counts = count_val(sarc_df)\n",
    "print(\"Sentiment split counts:\", sent_split_counts)\n",
    "print(\"Sarcasm  split counts:\", sarc_split_counts)\n",
    "print(f\"Sentiment validation size: {sent_val_n}\")\n",
    "print(f\"Sarcasm  validation size: {sarc_val_n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74657a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading: meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [03:20<00:00, 50.16s/it] \n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## EVAL: SENTIMENT (validation only) ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment (validation):   0%|          | 0/8101 [00:00<?, ?sample/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Sentiment (validation):   0%|          | 1/8101 [07:42<1040:14:33, 462.33s/sample]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "tokenizer, model = load_model(MODEL_ID)\n",
    "\n",
    "print(\"\\n########## EVAL: SENTIMENT (validation only) ##########\")\n",
    "res_sent = run_eval_on_validation(sent_df, \"sentiment\", tokenizer, model, batch_size=8)\n",
    "if res_sent is not None:\n",
    "    print(f\"\\n=== SENTIMENT | validation | N={res_sent['n']} ===\")\n",
    "    print(f\"Accuracy: {res_sent['accuracy']:.4f} | Macro-F1: {res_sent['macro_f1']:.4f}\")\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(np.array(res_sent[\"confusion_matrix\"]))\n",
    "    print(res_sent[\"report\"])\n",
    "\n",
    "    with open(os.path.join(OUT_DIR, \"sentiment_results_validation.json\"), \"w\") as f:\n",
    "        json.dump(res_sent, f, indent=2)\n",
    "    print(\"[OK] saved:\", os.path.join(OUT_DIR, \"sentiment_results_validation.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "print(\"\\n########## EVAL: SARCASM (validation only) ##########\")\n",
    "res_sarc = run_eval_on_validation(sarc_df, \"sarcasm\", tokenizer, model, batch_size=8)\n",
    "if res_sarc is not None:\n",
    "    print(f\"\\n=== SARCASM | validation | N={res_sarc['n']} ===\")\n",
    "    print(f\"Accuracy: {res_sarc['accuracy']:.4f} | Macro-F1: {res_sarc['macro_f1']:.4f}\")\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(np.array(res_sarc[\"confusion_matrix\"]))\n",
    "    print(res_sarc[\"report\"])\n",
    "\n",
    "    with open(os.path.join(OUT_DIR, \"sarcasm_results_validation.json\"), \"w\") as f:\n",
    "        json.dump(res_sarc, f, indent=2)\n",
    "    print(\"[OK] saved:\", os.path.join(OUT_DIR, \"sarcasm_results_validation.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f66b0286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment split counts: {'train': 254159, 'test': 19494, 'validation': 8101}\n",
      "Sarcasm  split counts: {'train': 11009, 'test': 2414, 'validation': 955}\n",
      "[INFO] Loading model: meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## EVAL: SENTIMENT (validation only) ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment (validation):   0%|          | 0/8101 [00:00<?, ?sample/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Sentiment (validation): 100%|██████████| 8101/8101 [56:18<00:00,  2.40sample/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SENTIMENT | validation | N=8101 ===\n",
      "Accuracy: 0.5586 | Macro-F1: 0.4956\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[ 878  483 1379]\n",
      " [ 114  573 1411]\n",
      " [   8  181 3074]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8780    0.3204    0.4695      2740\n",
      "     neutral     0.4632    0.2731    0.3436      2098\n",
      "    positive     0.5242    0.9421    0.6736      3263\n",
      "\n",
      "    accuracy                         0.5586      8101\n",
      "   macro avg     0.6218    0.5119    0.4956      8101\n",
      "weighted avg     0.6281    0.5586    0.5191      8101\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type int64 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 301\u001b[39m\n\u001b[32m    297\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[OK] saved:\u001b[39m\u001b[33m\"\u001b[39m, out_sarc)\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 279\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    277\u001b[39m     out_sent = os.path.join(args.out_dir, \u001b[33m\"\u001b[39m\u001b[33msentiment_results_validation.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_sent, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m         \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres_sent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[OK] saved:\u001b[39m\u001b[33m\"\u001b[39m, out_sent)\n\u001b[32m    282\u001b[39m \u001b[38;5;66;03m# Sarcasm (validation only)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llama-8B/lib/python3.11/json/__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llama-8B/lib/python3.11/json/encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llama-8B/lib/python3.11/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llama-8B/lib/python3.11/json/encoder.py:326\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_list\u001b[39m\u001b[34m(lst, _current_indent_level)\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    325\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    328\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llama-8B/lib/python3.11/json/encoder.py:439\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m o = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llama-8B/lib/python3.11/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type int64 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# eval_llama_validation.py\n",
    "# 读取 data/processed/sentiment_df.csv & sarcasm_df.csv\n",
    "# 只在 split=validation 上评测 sentiment(0/1/2) 和 sarcasm(0/1)\n",
    "# 结果保存到 --out_dir 下的 *_results_validation.json\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Config: device & dtype\n",
    "# -----------------------\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# 检测顺序：CUDA > MPS > CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"; DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\";  DTYPE = torch.float16\n",
    "else:\n",
    "    DEVICE = \"cpu\";  DTYPE = torch.float32\n",
    "\n",
    "SENTIMENT_LABELS = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "SARCASM_LABELS   = {0: \"non-sarcastic\", 1: \"sarcastic\"}\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Split helpers\n",
    "# -----------------------\n",
    "def normalize_split(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    t = str(s).strip().lower()\n",
    "    if t in {\"val\", \"valid\", \"validation\", \"dev\"}: return \"validation\"\n",
    "    if t in {\"train\", \"training\"}:                 return \"train\"\n",
    "    if t in {\"test\", \"testing\"}:                   return \"test\"\n",
    "    return t\n",
    "\n",
    "def filter_validation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"split\" not in df.columns:\n",
    "        return df.iloc[0:0].copy()\n",
    "    sub = df.copy()\n",
    "    sub[\"split_norm\"] = sub[\"split\"].map(normalize_split)\n",
    "    sub = sub[sub[\"split_norm\"] == \"validation\"].copy()\n",
    "    return sub.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Prompts\n",
    "# -----------------------\n",
    "def build_chat_prompt_sentiment(text: str):\n",
    "    system = (\n",
    "        \"You are a precise sentiment classifier. \"\n",
    "        \"Classify the user's message into one of three classes: \"\n",
    "        \"0=negative, 1=neutral, 2=positive. \"\n",
    "        \"Return exactly one digit (0 or 1 or 2). Output nothing else.\"\n",
    "    )\n",
    "    user = f\"Message:\\n{text}\\n\\nReturn only one digit (0/1/2).\"\n",
    "    return system, user\n",
    "\n",
    "def build_chat_prompt_sarcasm(text: str):\n",
    "    system = (\n",
    "        \"You are a precise sarcasm detector. \"\n",
    "        \"Decide if the user's message is sarcastic. \"\n",
    "        \"Use 0=non-sarcastic, 1=sarcastic. \"\n",
    "        \"Return exactly one digit (0 or 1). Output nothing else.\"\n",
    "    )\n",
    "    user = f\"Message:\\n{text}\\n\\nReturn only one digit (0/1).\"\n",
    "    return system, user\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Parsers\n",
    "# -----------------------\n",
    "_digit012 = re.compile(r\"([012])\")\n",
    "_digit01  = re.compile(r\"([01])\")\n",
    "\n",
    "def parse_digit_012(s: str) -> int | None:\n",
    "    if s is None: return None\n",
    "    t = s.strip()\n",
    "    if t in {\"0\",\"1\",\"2\"}: return int(t)\n",
    "    m = _digit012.search(t)\n",
    "    if m: return int(m.group(1))\n",
    "    lo = t.lower()\n",
    "    if \"negative\" in lo: return 0\n",
    "    if \"neutral\"  in lo: return 1\n",
    "    if \"positive\" in lo: return 2\n",
    "    return None\n",
    "\n",
    "def parse_digit_01(s: str) -> int | None:\n",
    "    if s is None: return None\n",
    "    t = s.strip()\n",
    "    if t in {\"0\",\"1\"}: return int(t)\n",
    "    m = _digit01.search(t)\n",
    "    if m: return int(m.group(1))\n",
    "    lo = t.lower()\n",
    "    if \"non-sarcastic\" in lo or \"non sarcastic\" in lo: return 0\n",
    "    if \"sarcastic\" in lo: return 1\n",
    "    return None\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Model loading & generate\n",
    "# -----------------------\n",
    "def load_model(model_name: str = DEFAULT_MODEL):\n",
    "    print(f\"[INFO] Loading model: {model_name}\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # 关键：decoder-only 用左填充 + 左截断\n",
    "    tok.padding_side = \"left\"\n",
    "    tok.truncation_side = \"left\"\n",
    "    if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    # 明确 device_map，MPS 上用 eager 更稳（不要 flash_attn）\n",
    "    if DEVICE == \"cuda\":\n",
    "        device_map = \"auto\"\n",
    "        attn_impl = \"flash_attention_2\"  # 仅 CUDA 可用；如果报错就换 \"eager\"\n",
    "    elif DEVICE == \"mps\":\n",
    "        device_map = {\"\": \"mps\"}\n",
    "        attn_impl = \"eager\"\n",
    "    else:\n",
    "        device_map = {\"\": \"cpu\"}\n",
    "        attn_impl = \"eager\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        dtype=DTYPE,\n",
    "        device_map=device_map,\n",
    "        low_cpu_mem_usage=True,\n",
    "        attn_implementation=attn_impl,\n",
    "    )\n",
    "    return tok, model\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_batch(tokenizer, model, systems, users, max_new_tokens=2):\n",
    "    prompts = []\n",
    "    for sys, usr in zip(systems, users):\n",
    "        messages = [{\"role\":\"system\",\"content\":sys}, {\"role\":\"user\",\"content\":usr}]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,             # 现在会用左填充\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).to(model.device)\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,          # 贪心；不打印 top_p/temperature 的 warn 了\n",
    "        pad_token_id=tokenizer.pad_token_id,   # 防止每次 fallback 提示\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    gen_tokens = out[:, inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Eval (validation only)\n",
    "# -----------------------\n",
    "def run_eval_validation(df: pd.DataFrame, task: str, tokenizer, model, batch_size=8, desc=\"\"):\n",
    "    \"\"\"\n",
    "    只评测 validation。\n",
    "      sentiment 需要列: text, sentiment, split\n",
    "      sarcasm   需要列: text, sarcasm,  split\n",
    "    \"\"\"\n",
    "    assert task in {\"sentiment\",\"sarcasm\"}\n",
    "    label_col = \"sentiment\" if task==\"sentiment\" else \"sarcasm\"\n",
    "    id2name   = SENTIMENT_LABELS if task==\"sentiment\" else SARCASM_LABELS\n",
    "    parser    = parse_digit_012 if task==\"sentiment\" else parse_digit_01\n",
    "    build     = build_chat_prompt_sentiment if task==\"sentiment\" else build_chat_prompt_sarcasm\n",
    "\n",
    "    sub = filter_validation(df)\n",
    "    if sub.empty:\n",
    "        print(f\"[Skip] No validation rows for task={task}.\")\n",
    "        return None\n",
    "\n",
    "    preds, gts = [], []\n",
    "    systems, users, idx_buf = [], [], []\n",
    "    pbar = tqdm(total=len(sub), desc=desc or f\"{task.capitalize()} (validation)\", unit=\"sample\")\n",
    "\n",
    "    for i, row in sub.iterrows():\n",
    "        sys, usr = build(row[\"text\"])\n",
    "        systems.append(sys); users.append(usr); idx_buf.append(i)\n",
    "\n",
    "        if len(systems) >= batch_size:\n",
    "            outs = generate_batch(tokenizer, model, systems, users)\n",
    "            for j, out in enumerate(outs):\n",
    "                p = parser(out)\n",
    "                if p is None: p = 1 if task==\"sentiment\" else 0\n",
    "                preds.append(p)\n",
    "                gts.append(int(sub.iloc[idx_buf[j]][label_col]))\n",
    "                pbar.update(1)\n",
    "            systems, users, idx_buf = [], [], []\n",
    "\n",
    "    if systems:\n",
    "        outs = generate_batch(tokenizer, model, systems, users)\n",
    "        for j, out in enumerate(outs):\n",
    "            p = parser(out)\n",
    "            if p is None: p = 1 if task==\"sentiment\" else 0\n",
    "            preds.append(p)\n",
    "            gts.append(int(sub.iloc[idx_buf[j]][label_col]))\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    preds = np.array(preds); gts = np.array(gts)\n",
    "    labels = sorted(set(gts) | set(preds))\n",
    "    cm = confusion_matrix(gts, preds, labels=labels)\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    macro_f1 = f1_score(gts, preds, average=\"macro\")\n",
    "    rep = classification_report(gts, preds, target_names=[id2name[i] for i in labels], digits=4)\n",
    "\n",
    "    return {\n",
    "        \"labels\": labels,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"report\": rep,\n",
    "        \"n\": len(sub),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Main\n",
    "# -----------------------\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--model\", type=str, default=DEFAULT_MODEL)\n",
    "    ap.add_argument(\"--bsz\", type=int, default=8)\n",
    "    ap.add_argument(\"--sentiment_csv\", type=str, default=\"../data/processed/sentiment_df.csv\")\n",
    "    ap.add_argument(\"--sarcasm_csv\",  type=str, default=\"../data/processed/sarcasm_df.csv\")\n",
    "    ap.add_argument(\"--out_dir\", type=str, default=\"runs/llama31_zeroshot_eval\")\n",
    "    args, _ = ap.parse_known_args()\n",
    "\n",
    "    # Load data\n",
    "    sent_df = pd.read_csv(args.sentiment_csv)\n",
    "    sarc_df = pd.read_csv(args.sarcasm_csv)\n",
    "    assert {\"text\",\"sentiment\",\"split\"}.issubset(sent_df.columns), f\"Bad columns in {args.sentiment_csv}: {sent_df.columns}\"\n",
    "    assert {\"text\",\"sarcasm\",\"split\"}.issubset(sarc_df.columns),  f\"Bad columns in {args.sarcasm_csv}: {sarc_df.columns}\"\n",
    "\n",
    "    # Show split counts\n",
    "    def split_counts(df):\n",
    "        return df[\"split\"].map(normalize_split).value_counts().to_dict()\n",
    "    print(\"Sentiment split counts:\", split_counts(sent_df))\n",
    "    print(\"Sarcasm  split counts:\", split_counts(sarc_df))\n",
    "\n",
    "    # Load model\n",
    "    tokenizer, model = load_model(args.model)\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "    # Sentiment (validation only)\n",
    "    print(\"\\n########## EVAL: SENTIMENT (validation only) ##########\")\n",
    "    res_sent = run_eval_validation(\n",
    "        sent_df, \"sentiment\", tokenizer, model,\n",
    "        batch_size=args.bsz, desc=\"Sentiment (validation)\"\n",
    "    )\n",
    "    if res_sent is not None:\n",
    "        print(f\"\\n=== SENTIMENT | validation | N={res_sent['n']} ===\")\n",
    "        print(f\"Accuracy: {res_sent['accuracy']:.4f} | Macro-F1: {res_sent['macro_f1']:.4f}\")\n",
    "        print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "        print(np.array(res_sent[\"confusion_matrix\"]))\n",
    "        print(res_sent[\"report\"])\n",
    "        out_sent = os.path.join(args.out_dir, \"sentiment_results_validation.json\")\n",
    "        with open(out_sent, \"w\") as f:\n",
    "            json.dump(res_sent, f, indent=2)\n",
    "        print(\"[OK] saved:\", out_sent)\n",
    "\n",
    "    # Sarcasm (validation only)\n",
    "    print(\"\\n########## EVAL: SARCASM (validation only) ##########\")\n",
    "    res_sarc = run_eval_validation(\n",
    "        sarc_df, \"sarcasm\", tokenizer, model,\n",
    "        batch_size=args.bsz, desc=\"Sarcasm (validation)\"\n",
    "    )\n",
    "    if res_sarc is not None:\n",
    "        print(f\"\\n=== SARCASM | validation | N={res_sarc['n']} ===\")\n",
    "        print(f\"Accuracy: {res_sarc['accuracy']:.4f} | Macro-F1: {res_sarc['macro_f1']:.4f}\")\n",
    "        print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "        print(np.array(res_sarc[\"confusion_matrix\"]))\n",
    "        print(res_sarc[\"report\"])\n",
    "        out_sarc = os.path.join(args.out_dir, \"sarcasm_results_validation.json\")\n",
    "        with open(out_sarc, \"w\") as f:\n",
    "            json.dump(res_sarc, f, indent=2)\n",
    "        print(\"[OK] saved:\", out_sarc)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac63556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment split counts: {'train': 254159, 'test': 19494, 'validation': 8101}\n",
      "Sarcasm  split counts: {'train': 11009, 'test': 2414, 'validation': 955}\n",
      "[INFO] Loading model: meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## EVAL: SENTIMENT (validation only) ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment (validation):   0%|          | 0/8101 [00:00<?, ?sample/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Sentiment (validation): 100%|██████████| 8101/8101 [56:30<00:00,  2.39sample/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SENTIMENT | validation | N=8101 ===\n",
      "Accuracy: 0.5586 | Macro-F1: 0.4956\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[ 878  483 1379]\n",
      " [ 114  573 1411]\n",
      " [   8  181 3074]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8780    0.3204    0.4695      2740\n",
      "     neutral     0.4632    0.2731    0.3436      2098\n",
      "    positive     0.5242    0.9421    0.6736      3263\n",
      "\n",
      "    accuracy                         0.5586      8101\n",
      "   macro avg     0.6218    0.5119    0.4956      8101\n",
      "weighted avg     0.6281    0.5586    0.5191      8101\n",
      "\n",
      "\n",
      "########## EVAL: SARCASM (validation only) ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sarcasm (validation): 100%|██████████| 955/955 [04:58<00:00,  3.20sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SARCASM | validation | N=955 ===\n",
      "Accuracy: 0.5110 | Macro-F1: 0.3978\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[ 37 462]\n",
      " [  5 451]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "non-sarcastic     0.8810    0.0741    0.1368       499\n",
      "    sarcastic     0.4940    0.9890    0.6589       456\n",
      "\n",
      "     accuracy                         0.5110       955\n",
      "    macro avg     0.6875    0.5316    0.3978       955\n",
      " weighted avg     0.6962    0.5110    0.3861       955\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# eval_llama_validation_nosave.py\n",
    "# 读取 ../data/processed/sentiment_df.csv 和 ../data/processed/sarcasm_df.csv\n",
    "# 仅评测 split=validation 的样本；打印 Accuracy / Macro-F1 / 混淆矩阵与分类报告\n",
    "# 不保存任何 json 到磁盘\n",
    "\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# -----------------------\n",
    "# Config: device & dtype\n",
    "# -----------------------\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# 检测顺序：CUDA > MPS > CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"; DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\";  DTYPE = torch.float16\n",
    "else:\n",
    "    DEVICE = \"cpu\";  DTYPE = torch.float32\n",
    "\n",
    "SENTIMENT_LABELS = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "SARCASM_LABELS   = {0: \"non-sarcastic\", 1: \"sarcastic\"}\n",
    "\n",
    "# -----------------------\n",
    "# Split helpers\n",
    "# -----------------------\n",
    "def normalize_split(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    t = str(s).strip().lower()\n",
    "    if t in {\"val\", \"valid\", \"validation\", \"dev\"}: return \"validation\"\n",
    "    if t in {\"train\", \"training\"}:                 return \"train\"\n",
    "    if t in {\"test\", \"testing\"}:                   return \"test\"\n",
    "    return t\n",
    "\n",
    "def filter_validation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"split\" not in df.columns:\n",
    "        return df.iloc[0:0].copy()\n",
    "    sub = df.copy()\n",
    "    sub[\"split_norm\"] = sub[\"split\"].map(normalize_split)\n",
    "    sub = sub[sub[\"split_norm\"] == \"validation\"].copy()\n",
    "    return sub.reset_index(drop=True)\n",
    "\n",
    "# -----------------------\n",
    "# Prompts\n",
    "# -----------------------\n",
    "def build_chat_prompt_sentiment(text: str):\n",
    "    system = (\n",
    "        \"You are a precise sentiment classifier. \"\n",
    "        \"Classify the user's message into one of three classes: \"\n",
    "        \"0=negative, 1=neutral, 2=positive. \"\n",
    "        \"Return exactly one digit (0 or 1 or 2). Output nothing else.\"\n",
    "    )\n",
    "    user = f\"Message:\\n{text}\\n\\nReturn only one digit (0/1/2).\"\n",
    "    return system, user\n",
    "\n",
    "def build_chat_prompt_sarcasm(text: str):\n",
    "    system = (\n",
    "        \"You are a precise sarcasm detector. \"\n",
    "        \"Decide if the user's message is sarcastic. \"\n",
    "        \"Use 0=non-sarcastic, 1=sarcastic. \"\n",
    "        \"Return exactly one digit (0 or 1). Output nothing else.\"\n",
    "    )\n",
    "    user = f\"Message:\\n{text}\\n\\nReturn only one digit (0/1).\"\n",
    "    return system, user\n",
    "\n",
    "# -----------------------\n",
    "# Parsers\n",
    "# -----------------------\n",
    "_digit012 = re.compile(r\"([012])\")\n",
    "_digit01  = re.compile(r\"([01])\")\n",
    "\n",
    "def parse_digit_012(s: str) -> int | None:\n",
    "    if s is None: return None\n",
    "    t = s.strip()\n",
    "    if t in {\"0\",\"1\",\"2\"}: return int(t)\n",
    "    m = _digit012.search(t)\n",
    "    if m: return int(m.group(1))\n",
    "    lo = t.lower()\n",
    "    if \"negative\" in lo: return 0\n",
    "    if \"neutral\"  in lo: return 1\n",
    "    if \"positive\" in lo: return 2\n",
    "    return None\n",
    "\n",
    "def parse_digit_01(s: str) -> int | None:\n",
    "    if s is None: return None\n",
    "    t = s.strip()\n",
    "    if t in {\"0\",\"1\"}: return int(t)\n",
    "    m = _digit01.search(t)\n",
    "    if m: return int(m.group(1))\n",
    "    lo = t.lower()\n",
    "    if \"non-sarcastic\" in lo or \"non sarcastic\" in lo: return 0\n",
    "    if \"sarcastic\" in lo: return 1\n",
    "    return None\n",
    "\n",
    "# -----------------------\n",
    "# Model loading & generate\n",
    "# -----------------------\n",
    "def load_model(model_name: str = DEFAULT_MODEL):\n",
    "    print(f\"[INFO] Loading model: {model_name}\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # decoder-only: 左填充 + 左截断\n",
    "    tok.padding_side = \"left\"\n",
    "    tok.truncation_side = \"left\"\n",
    "    if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    # attn_implementation 在不同版本 transformers 里可选\n",
    "    if DEVICE == \"cuda\":\n",
    "        device_map = \"auto\"\n",
    "        attn_impl = \"flash_attention_2\"\n",
    "    elif DEVICE == \"mps\":\n",
    "        device_map = {\"\": \"mps\"}\n",
    "        attn_impl = \"eager\"\n",
    "    else:\n",
    "        device_map = {\"\": \"cpu\"}\n",
    "        attn_impl = \"eager\"\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            dtype=DTYPE,\n",
    "            device_map=device_map,\n",
    "            low_cpu_mem_usage=True,\n",
    "            attn_implementation=attn_impl,\n",
    "        )\n",
    "    except TypeError:\n",
    "        # 兼容老版本 transformers（没有 attn_implementation 参数）\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            dtype=DTYPE,\n",
    "            device_map=device_map,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "    return tok, model\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_batch(tokenizer, model, systems, users, max_new_tokens=2):\n",
    "    prompts = []\n",
    "    for sys, usr in zip(systems, users):\n",
    "        messages = [{\"role\":\"system\",\"content\":sys}, {\"role\":\"user\",\"content\":usr}]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,      # 左填充\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).to(model.device)\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    gen_tokens = out[:, inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "\n",
    "# -----------------------\n",
    "# Eval (validation only)\n",
    "# -----------------------\n",
    "def run_eval_validation(df: pd.DataFrame, task: str, tokenizer, model, batch_size=8, desc=\"\"):\n",
    "    \"\"\"\n",
    "    只评测 validation。\n",
    "      sentiment 需要列: text, sentiment, split\n",
    "      sarcasm   需要列: text, sarcasm,  split\n",
    "    \"\"\"\n",
    "    assert task in {\"sentiment\",\"sarcasm\"}\n",
    "    label_col = \"sentiment\" if task==\"sentiment\" else \"sarcasm\"\n",
    "    id2name   = SENTIMENT_LABELS if task==\"sentiment\" else SARCASM_LABELS\n",
    "    parser    = parse_digit_012 if task==\"sentiment\" else parse_digit_01\n",
    "    build     = build_chat_prompt_sentiment if task==\"sentiment\" else build_chat_prompt_sarcasm\n",
    "\n",
    "    sub = filter_validation(df)\n",
    "    if sub.empty:\n",
    "        print(f\"[Skip] No validation rows for task={task}.\")\n",
    "        return None\n",
    "\n",
    "    preds, gts = [], []\n",
    "    systems, users, idx_buf = [], [], []\n",
    "    pbar = tqdm(total=len(sub), desc=desc or f\"{task.capitalize()} (validation)\", unit=\"sample\")\n",
    "\n",
    "    for i, row in sub.iterrows():\n",
    "        sys, usr = build(row[\"text\"])\n",
    "        systems.append(sys); users.append(usr); idx_buf.append(i)\n",
    "\n",
    "        if len(systems) >= batch_size:\n",
    "            outs = generate_batch(tokenizer, model, systems, users)\n",
    "            for j, out in enumerate(outs):\n",
    "                p = parser(out)\n",
    "                if p is None: p = 1 if task==\"sentiment\" else 0\n",
    "                preds.append(p)\n",
    "                gts.append(int(sub.iloc[idx_buf[j]][label_col]))\n",
    "                pbar.update(1)\n",
    "            systems, users, idx_buf = [], [], []\n",
    "\n",
    "    if systems:\n",
    "        outs = generate_batch(tokenizer, model, systems, users)\n",
    "        for j, out in enumerate(outs):\n",
    "            p = parser(out)\n",
    "            if p is None: p = 1 if task==\"sentiment\" else 0\n",
    "            preds.append(p)\n",
    "            gts.append(int(sub.iloc[idx_buf[j]][label_col]))\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    preds = np.array(preds); gts = np.array(gts)\n",
    "    labels = sorted(set(gts) | set(preds))\n",
    "    cm = confusion_matrix(gts, preds, labels=labels)\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    macro_f1 = f1_score(gts, preds, average=\"macro\")\n",
    "    rep = classification_report(gts, preds, target_names=[id2name[i] for i in labels], digits=4)\n",
    "\n",
    "    # 直接返回可打印对象（不做 JSON 序列化）\n",
    "    return {\n",
    "        \"labels\": [int(x) for x in labels],\n",
    "        \"confusion_matrix\": cm.astype(int),\n",
    "        \"accuracy\": float(acc),\n",
    "        \"macro_f1\": float(macro_f1),\n",
    "        \"report\": rep,\n",
    "        \"n\": int(len(sub)),\n",
    "    }\n",
    "\n",
    "# -----------------------\n",
    "# Main\n",
    "# -----------------------\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--model\", type=str, default=DEFAULT_MODEL)\n",
    "    ap.add_argument(\"--bsz\", type=int, default=8)\n",
    "    ap.add_argument(\"--sentiment_csv\", type=str, default=\"../data/processed/sentiment_df.csv\")\n",
    "    ap.add_argument(\"--sarcasm_csv\",  type=str, default=\"../data/processed/sarcasm_df.csv\")\n",
    "    args, _ = ap.parse_known_args()\n",
    "\n",
    "    # Load data\n",
    "    sent_df = pd.read_csv(args.sentiment_csv)\n",
    "    sarc_df = pd.read_csv(args.sarcasm_csv)\n",
    "    assert {\"text\",\"sentiment\",\"split\"}.issubset(sent_df.columns), f\"Bad columns in {args.sentiment_csv}: {sent_df.columns}\"\n",
    "    assert {\"text\",\"sarcasm\",\"split\"}.issubset(sarc_df.columns),  f\"Bad columns in {args.sarcasm_csv}: {sarc_df.columns}\"\n",
    "\n",
    "    # Show split counts\n",
    "    def split_counts(df):\n",
    "        return df[\"split\"].map(normalize_split).value_counts().to_dict()\n",
    "    print(\"Sentiment split counts:\", split_counts(sent_df))\n",
    "    print(\"Sarcasm  split counts:\", split_counts(sarc_df))\n",
    "\n",
    "    # Load model\n",
    "    tokenizer, model = load_model(args.model)\n",
    "\n",
    "    # Sentiment (validation only)\n",
    "    print(\"\\n########## EVAL: SENTIMENT (validation only) ##########\")\n",
    "    res_sent = run_eval_validation(\n",
    "        sent_df, \"sentiment\", tokenizer, model,\n",
    "        batch_size=args.bsz, desc=\"Sentiment (validation)\"\n",
    "    )\n",
    "    if res_sent is not None:\n",
    "        print(f\"\\n=== SENTIMENT | validation | N={res_sent['n']} ===\")\n",
    "        print(f\"Accuracy: {res_sent['accuracy']:.4f} | Macro-F1: {res_sent['macro_f1']:.4f}\")\n",
    "        print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "        print(np.array(res_sent[\"confusion_matrix\"]))\n",
    "        print(res_sent[\"report\"])\n",
    "\n",
    "    # Sarcasm (validation only)\n",
    "    print(\"\\n########## EVAL: SARCASM (validation only) ##########\")\n",
    "    res_sarc = run_eval_validation(\n",
    "        sarc_df, \"sarcasm\", tokenizer, model,\n",
    "        batch_size=args.bsz, desc=\"Sarcasm (validation)\"\n",
    "    )\n",
    "    if res_sarc is not None:\n",
    "        print(f\"\\n=== SARCASM | validation | N={res_sarc['n']} ===\")\n",
    "        print(f\"Accuracy: {res_sarc['accuracy']:.4f} | Macro-F1: {res_sarc['macro_f1']:.4f}\")\n",
    "        print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "        print(np.array(res_sarc[\"confusion_matrix\"]))\n",
    "        print(res_sarc[\"report\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c96e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa1abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-8B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
